{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Controversit: a tool to identify potentially controversial submissions on reddit\n",
    "\n",
    "This version is for exploration and comes in Jupyter notebook format for ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:07.772711Z",
     "start_time": "2019-09-24T19:51:06.481801Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from itertools import starmap\n",
    "import multiprocessing as mp\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:07.779605Z",
     "start_time": "2019-09-24T19:51:07.774663Z"
    }
   },
   "outputs": [],
   "source": [
    "if path.exists(\"subreddit_data\"):\n",
    "    print(\"Directory subreddit_data exists!\")\n",
    "else :\n",
    "    !mkdir subreddit_data\n",
    "    print(\"Directory subreddit_data created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Crawling with PRAW: a python wrapper for the Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T04:33:20.383301Z",
     "start_time": "2019-09-17T04:33:19.267578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "uname = \"\"\n",
    "upassword = \"\"\n",
    "uagent = \"controversit by /u/\"+uname\n",
    "cl_id = \"\"\n",
    "cl_secret = \"\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=cl_id,\n",
    "                     client_secret=cl_secret,\n",
    "                     password=upassword,\n",
    "                     user_agent=uagent,\n",
    "                     username=uname)\n",
    "\n",
    "print(\"Logged in as user: \",reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Choose a subreddit and get a list of submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T00:40:22.700812Z",
     "start_time": "2019-09-13T00:40:22.688846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_pd_dframe(nrows,list_of_fields):\n",
    "    '''\n",
    "    A simple function to create an empty\n",
    "    dataframe with a given number of rows \n",
    "    and a given list of columns\n",
    "    '''\n",
    "    \n",
    "    shape = (nrows,len(list_of_fields))\n",
    "    d = np.empty(shape)\n",
    "    df = pd.DataFrame(data=d,columns=list_of_fields)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def submission_data_to_dframe(sorting_scheme,n_subm=100):\n",
    "    '''\n",
    "    A function that gets n_subm reddit submissions, \n",
    "    retrieves their properties, comments to the \n",
    "    submission, then saves the results into a pandas \n",
    "    dataframe.\n",
    "    '''\n",
    "\n",
    "    # Initialize pandas dataframe\n",
    "    list_of_fields = [\"subm_ID\",\"subm_title\",\"subm_author\", \\\n",
    "                      \"subm_created_utc\",\"subm_upvote_ratio\", \\\n",
    "                      \"subm_link_flair_text\", \\\n",
    "                      \"subm_comment_ids\", \\\n",
    "                      \"subm_comment_authors\", \\\n",
    "                      \"subm_comment_bodies\", \\\n",
    "                      \"subm_comment_scores\"]\n",
    "    df = create_pd_dframe(n_subm,list_of_fields)\n",
    "    df[\"subm_comment_ids\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_authors\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_bodies\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_scores\"] = [[]] * len(df)\n",
    "        \n",
    "    # Use reddit api to retrieve the necessary information.\n",
    "    i = 0 \n",
    "    for sub in sorting_scheme(limit=n_subm):\n",
    "\n",
    "        # Data on submissions\n",
    "        df[\"subm_ID\"].iloc[i] = sub.id\n",
    "        df[\"subm_title\"].iloc[i] = sub.title\n",
    "        df[\"subm_author\"].iloc[i] = sub.author\n",
    "        df[\"subm_created_utc\"].iloc[i] = float(sub.created_utc)\n",
    "        df[\"subm_upvote_ratio\"].iloc[i] = float(sub.upvote_ratio)\n",
    "        df[\"subm_link_flair_text\"].iloc[i] = sub.link_flair_text\n",
    "        \n",
    "        # Data on comments\n",
    "        sub.comments.replace_more(limit=0)\n",
    "        comment_ids = []\n",
    "        comment_authors = []\n",
    "        comment_bodies = []\n",
    "        comment_scores = []\n",
    "        for com in sub.comments:\n",
    "            comment_ids.append(com.id)\n",
    "            comment_authors.append(com.author)\n",
    "            comment_bodies.append(com.body)\n",
    "            comment_scores.append(float(com.score))\n",
    "        df[\"subm_comment_ids\"].iloc[i] = comment_ids\n",
    "        df[\"subm_comment_authors\"].iloc[i] = comment_authors\n",
    "        df[\"subm_comment_bodies\"].iloc[i] = comment_bodies\n",
    "        df[\"subm_comment_scores\"].iloc[i] = comment_scores\n",
    "        \n",
    "        # Increase counter\n",
    "        i=i+1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:46:46.464469Z",
     "start_time": "2019-09-13T01:43:34.574765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose a subreddit and a number of submissions\n",
    "\n",
    "subreddit_name = \"history\"\n",
    "\n",
    "n_subm = 1000\n",
    "\n",
    "subreddit_obj = reddit.subreddit(subreddit_name)\n",
    "\n",
    "subm_data = submission_data_to_dframe(subreddit_obj.new,n_subm=n_subm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:47:04.112251Z",
     "start_time": "2019-09-13T01:47:04.086925Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display dataframe\n",
    "subm_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dump data into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:47:10.204254Z",
     "start_time": "2019-09-13T01:47:10.189359Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "timestamp = time.time()\n",
    "dati = datetime.datetime.fromtimestamp(timestamp)\n",
    "da = str(dati)[0:10]\n",
    "ti = str(dati)[11:19]\n",
    "\n",
    "fname = \"dump_r-\"+subreddit_name+\"_\"+da+\".pkl\"\n",
    "print(fname)\n",
    "subm_data.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Make a few diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:39:53.102630Z",
     "start_time": "2019-09-13T06:39:52.582606Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make an histogram of the controversial indicator \n",
    "# i.e. subm_upvote_ratio\n",
    "\n",
    "# Load a couple of \n",
    "df0 = pd.read_pickle(\"dump_r-politicaldiscussion_2019-09-12.pkl\")\n",
    "name0 = \"politicaldiscussion\"\n",
    "df1 = pd.read_pickle(\"dump_r-amitheasshole_2019-09-12.pkl\")\n",
    "name1 = \"amitheasshole\"\n",
    "df2 = pd.read_pickle(\"dump_r-changemyview_2019-09-12.pkl\")\n",
    "name2 = \"changemyview\"\n",
    "\n",
    "params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.axis([-0.05,1.05,0.1,200])\n",
    "plt.xlabel(\"Upvotes/(Upvotes+Downvotes)\")\n",
    "plt.xticks(np.arange(0,1.2,0.2),labels=[\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1.0\"])\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "plt.yticks([0.1,1,10,100],labels=[\"0.1\",\"1.0\",\"10\",\"100\"])\n",
    "plt.axvspan(0.25, 0.75, facecolor='gray',alpha=0.3,label=\"Controversial Range\")\n",
    "plt.plot([0.5,0.5],[0,1e8],\"k:\",linewidth=5,label=\"Most Controversial\")\n",
    "histdata = [df0[\"subm_upvote_ratio\"].values,df1[\"subm_upvote_ratio\"].values,df2[\"subm_upvote_ratio\"].values]\n",
    "\n",
    "colors = [\"red\",\"lightgreen\",\"lightblue\"]\n",
    "labels = [\"r/\"+name0,\"r/\"+name1,\"r/\"+name2]\n",
    "plt.hist(histdata,bins=10,range=[0,1],rwidth=0.8,label=labels,color=colors,edgecolor='black',alpha=1)\n",
    "plt.legend(loc='top left', bbox_to_anchor=(1.05, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:39:54.985069Z",
     "start_time": "2019-09-13T06:39:54.717219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Another interesting plot\n",
    "\n",
    "# Load a couple of \n",
    "df0 = pd.read_pickle(\"dump_r-politicaldiscussion_2019-09-12.pkl\")\n",
    "name0 = \"politicaldiscussion\"\n",
    "df1 = pd.read_pickle(\"dump_r-amitheasshole_2019-09-12.pkl\")\n",
    "name1 = \"amitheasshole\"\n",
    "df2 = pd.read_pickle(\"dump_r-changemyview_2019-09-12.pkl\")\n",
    "name2 = \"changemyview\"\n",
    "\n",
    "params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "sel0 = df0[\"subm_upvote_ratio\"] <= 0.25\n",
    "sel1 = df1[\"subm_upvote_ratio\"] <= 0.25\n",
    "sel2 = df2[\"subm_upvote_ratio\"] <= 0.25\n",
    "negative_opinion = np.array([ \\\n",
    "                    float(len(df0[\"subm_upvote_ratio\"][sel0]))/float(len(df0[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df1[\"subm_upvote_ratio\"][sel1]))/float(len(df1[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df2[\"subm_upvote_ratio\"][sel0]))/float(len(df2[\"subm_upvote_ratio\"])) \\\n",
    "                   ])\n",
    "\n",
    "sel0 = df0[\"subm_upvote_ratio\"] >= 0.75\n",
    "sel1 = df1[\"subm_upvote_ratio\"] >= 0.75\n",
    "sel2 = df2[\"subm_upvote_ratio\"] >= 0.75\n",
    "positive_opinion = np.array([ \\\n",
    "                    float(len(df0[\"subm_upvote_ratio\"][sel0]))/float(len(df0[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df1[\"subm_upvote_ratio\"][sel1]))/float(len(df1[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df2[\"subm_upvote_ratio\"][sel0]))/float(len(df2[\"subm_upvote_ratio\"])) \\\n",
    "                   ])\n",
    "\n",
    "controversial_opinion = np.array([1.0,1.0,1.0])\n",
    "controversial_opinion = controversial_opinion-positive_opinion-negative_opinion\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ind = [0,1,2]\n",
    "p1 = plt.barh(ind,negative_opinion,height=0.5,color=\"red\",label=\"Negative\")\n",
    "p2 = plt.barh(ind,controversial_opinion,height=0.5,color=\"orange\",left=negative_opinion,label=\"Controversial\")\n",
    "p3 = plt.barh(ind,positive_opinion,color=\"green\",height=0.5,\\\n",
    "     left=negative_opinion+controversial_opinion, label=\"Positive\")\n",
    "plt.xlabel(\"Fraction of Submissions\")\n",
    "plt.yticks(ind,[\"r/\"+name0,\"r/\"+name1,\"r/\"+name2])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## A few additional tests with PRAW: run a search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T21:11:40.613731Z",
     "start_time": "2019-09-16T21:11:34.764921Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A test with a different API call\n",
    "\n",
    "subreddit_name = \"history\"\n",
    "\n",
    "n_subm = 1000\n",
    "\n",
    "subreddit_obj = reddit.subreddit(subreddit_name)\n",
    "\n",
    "i=0\n",
    "for sub in subreddit_obj.search(\"all\", sort='new', syntax='lucene', time_filter='all', limit=10):\n",
    "    print(i,sub.title)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling with Pushshift.io\n",
    "\n",
    "More versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A series of functions to use the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:09.862021Z",
     "start_time": "2019-09-24T19:51:09.857650Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPushshiftData(after, before, limit, subr):\n",
    "    '''\n",
    "    This function queries submissions from a subreddit\n",
    "    within a given time range [before,after] using \n",
    "    a Pushshift.io call.\n",
    "    '''\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/?size=\"+ \\\n",
    "        str(limit)+\"&after=\"+str(after)+\"&before=\"+str(before)+\"&subreddit=\"+str(subr)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:10.166302Z",
     "start_time": "2019-09-24T19:51:10.161813Z"
    }
   },
   "outputs": [],
   "source": [
    "def getExtra(sub_id,useragent):\n",
    "    '''\n",
    "    Given a submission sub_id, get extra data\n",
    "    that is not provided by Pushshift.io, e.g.\n",
    "    score and upvote ratio.\n",
    "    '''\n",
    "    searchURL = 'http://reddit.com/'\n",
    "    url = searchURL + str(sub_id) + '.json'\n",
    "    r = requests.get(url, headers = {'User-agent': useragent})\n",
    "    extra_data = json.loads(r.text)\n",
    "    out0 = extra_data[0][\"data\"][\"children\"][0][\"data\"][\"score\"]\n",
    "    out1 = extra_data[0][\"data\"][\"children\"][0][\"data\"][\"upvote_ratio\"] \n",
    "    return out0,out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:10.504378Z",
     "start_time": "2019-09-24T19:51:10.496377Z"
    }
   },
   "outputs": [],
   "source": [
    "def collectSubData(subDic,subm,subr,useragent):\n",
    "    '''\n",
    "    This function collects data on submissions to a \n",
    "    given subreddit and saves them in a dictionary\n",
    "    '''\n",
    "    subDic[\"subreddit\"].append(subr)\n",
    "    subDic[\"sub_id\"].append(subm[\"id\"])\n",
    "    subDic[\"title\"].append(subm[\"title\"])\n",
    "    subDic[\"author\"].append(subm[\"author\"])\n",
    "    dt = str(datetime.datetime.fromtimestamp(subm[\"created_utc\"]))\n",
    "    subDic[\"created\"].append(dt)\n",
    "    subDic[\"url\"].append(subm[\"url\"])\n",
    "    subDic[\"permalink\"].append(subm[\"permalink\"])\n",
    "    try:\n",
    "        subDic[\"flair\"].append(subm[\"link_flair_text\"])\n",
    "    except KeyError:\n",
    "        subDic[\"flair\"].append(\"NaN\")\n",
    "    subDic[\"numComms\"].append(subm[\"num_comments\"])\n",
    "    try:\n",
    "        subDic[\"selftext\"].append(subm[\"selftext\"])\n",
    "    except KeyError:    \n",
    "        subDic[\"selftext\"].append(\"NaN\")\n",
    "    try:\n",
    "        score,upvote_ratio = getExtra(subm[\"id\"],useragent)\n",
    "        subDic[\"upvote_ratio\"].append(upvote_ratio)\n",
    "        subDic[\"score\"].append(score)\n",
    "    except:\n",
    "        subDic[\"upvote_ratio\"].append(\"Nan\")\n",
    "        subDic[\"score\"].append(subm[\"score\"])\n",
    "\n",
    "    return subDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:10.917542Z",
     "start_time": "2019-09-24T19:51:10.912838Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runQuery(subCount, subDic,after, before, limit, subr, useragent):\n",
    "    '''\n",
    "    This function calls the appropriate functions to \n",
    "    run a Pushshift.io query to get submissions in \n",
    "    a given subreddit, then calls saveQueryData to \n",
    "    save data to disk in pickle format using pandas.\n",
    "    '''\n",
    "    # Run query\n",
    "    data = getPushshiftData(after, before, limit, subr)\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            subDic = collectSubData(subDic,submission,subr,useragent)\n",
    "            subCount+=1\n",
    "        after = data[-1][\"created_utc\"]\n",
    "        data = getPushshiftData(after, before, limit, subr)\n",
    "    \n",
    "    df = pd.DataFrame(data=subDic)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:11.339556Z",
     "start_time": "2019-09-24T19:51:11.336365Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveDFrame(df,subr):\n",
    "    outpath = \"./subreddit_data/\"\n",
    "    fname = outpath+\"r-\"+subr+\"-export.pkl\"\n",
    "    df.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:11.911023Z",
     "start_time": "2019-09-24T19:51:11.905634Z"
    }
   },
   "outputs": [],
   "source": [
    "def MPQueryWrapper(after,before,limit,subr,useragent):\n",
    "    '''\n",
    "    This functions wrap API calls for use with the \n",
    "    multiprocessing package. With this function \n",
    "    multiple queries can be run in parallel. \n",
    "    '''\n",
    "    # Fields that will be used\n",
    "    fields = [\"subreddit\",\"sub_id\",\"title\",\"author\", \\\n",
    "          \"created\",\"url\",\"permalink\",\"score\", \\\n",
    "          \"numComms\",\"flair\",\"selftext\",\"upvote_ratio\"]\n",
    "    subCount = 0\n",
    "    subDic = {}\n",
    "    for f in fields:\n",
    "        subDic[f] = [] \n",
    "    print(after,before,limit,subr,useragent)\n",
    "    df = runQuery(subCount,subDic,after,before,limit,subr,useragent)\n",
    "    \n",
    "    saveDFrame(df,subr)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:12.475896Z",
     "start_time": "2019-09-24T19:51:12.472620Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveDFrameT(df,subr,coreid):\n",
    "    outpath = \"./subreddit_data/\"\n",
    "    fname = outpath+\"r-\"+subr+\"-export-\"+coreid+\".pkl\"\n",
    "    df.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:13.102095Z",
     "start_time": "2019-09-24T19:51:13.096730Z"
    }
   },
   "outputs": [],
   "source": [
    "def MPQueryWrapperT(after,before,limit,subr,useragent):\n",
    "    '''\n",
    "    This functions wrap API calls for use with the \n",
    "    multiprocessing package. With this function \n",
    "    multiple queries can be run in parallel. \n",
    "    '''\n",
    "    # Fields that will be used\n",
    "    fields = [\"subreddit\",\"sub_id\",\"title\",\"author\", \\\n",
    "          \"created\",\"url\",\"permalink\",\"score\", \\\n",
    "          \"numComms\",\"flair\",\"selftext\",\"upvote_ratio\"]\n",
    "    subCount = 0\n",
    "    subDic = {}\n",
    "    for f in fields:\n",
    "        subDic[f] = [] \n",
    "    print(after,before,limit,subr,useragent)\n",
    "    df = runQuery(subCount,subDic,after,before,limit,subr,useragent)\n",
    "    \n",
    "    coreid = mp.current_process()\n",
    "    saveDFrameT(df,subr,coreid)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose list of subreddits, a time range and a list of query parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:49:12.601854Z",
     "start_time": "2019-09-24T19:49:12.596461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useragent\n",
    "useragent = \"\"\n",
    "\n",
    "# Subreddit selection\n",
    "subr_list = [\"history\"]\n",
    "\n",
    "# Max number of submissions per query\n",
    "limit = 10000\n",
    "\n",
    "# Time selection based on timestamps\n",
    "# This currently selects all posts\n",
    "#dateini = \"2019-01-01\" # Fiducial for advice, changemyview, confession, parenting\n",
    "dateini = \"2010-01-01\"\n",
    "dateend = \"2020-01-01\"\n",
    "tsi = ciso8601.parse_datetime(dateini)\n",
    "tsf = ciso8601.parse_datetime(dateend)\n",
    "# to get time in seconds:\n",
    "after = int(time.mktime(tsi.timetuple()))\n",
    "before = int(time.mktime(tsf.timetuple()))\n",
    "\n",
    "query_params = []\n",
    "for i in range(0,len(subr_list)):\n",
    "    query_params.append((after,before,limit,subr_list[i],useragent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:49:38.893666Z",
     "start_time": "2019-09-24T19:49:14.020324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run on multiple cores\n",
    "\n",
    "ncores = min(mp.cpu_count(),len(subr_list))\n",
    "worker_pool = mp.Pool(ncores)\n",
    "\n",
    "out = []\n",
    "for df in worker_pool.starmap(MPQueryWrapper, query_params):\n",
    "    out.append(df)\n",
    "        \n",
    "#Close threads\n",
    "worker_pool.close()\n",
    "worker_pool.join()\n",
    "\n",
    "display(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative setup for very large subreddits: split time range in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T19:51:26.982682Z",
     "start_time": "2019-09-24T19:51:26.976451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useragent\n",
    "useragent = \"\"\n",
    "\n",
    "# Subreddit selection\n",
    "subrname = \"history\"\n",
    "\n",
    "# Max number of submissions per query\n",
    "limit = 10000\n",
    "\n",
    "# Time selection based on timestamps\n",
    "# This currently selects all posts\n",
    "dateini = \"2010-01-01\"\n",
    "dateend = \"2020-01-01\"\n",
    "tsi = ciso8601.parse_datetime(dateini)\n",
    "tsf = ciso8601.parse_datetime(dateend)\n",
    "# to get time in seconds:\n",
    "dateini = int(time.mktime(tsi.timetuple()))\n",
    "dateend = int(time.mktime(tsf.timetuple()))\n",
    "\n",
    "nchunks = 8\n",
    "dt = int((dateend-dateini)/nchunks)\n",
    "query_params = []\n",
    "for i in range(0,nchunks):\n",
    "    after = dateini+int(i*dt)\n",
    "    if i == nchunks-1 :\n",
    "        before = dateend\n",
    "    else :\n",
    "        before = dateini+int((i+1)*dt)\n",
    "    query_params.append((after,before,limit,subrname,useragent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run API query on the time chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T19:51:28.718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run on multiple cores\n",
    "from itertools import starmap\n",
    "import multiprocessing as mp\n",
    "\n",
    "ncores = min(mp.cpu_count(),nchunks)\n",
    "worker_pool = mp.Pool(ncores)\n",
    "\n",
    "out = []\n",
    "for df in worker_pool.starmap(MPQueryWrapperT, query_params):\n",
    "    out.append(df)        \n",
    "\n",
    "#Close threads\n",
    "worker_pool.close()\n",
    "worker_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
