{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Controversit: a tool to identify potentially controversial submissions on reddit\n",
    "\n",
    "This version is for exploration and comes in Jupyter notebook format for ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:06:56.000137Z",
     "start_time": "2019-09-22T08:06:54.537203Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:06:56.763154Z",
     "start_time": "2019-09-22T08:06:56.758793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory subreddit_data exists!\n"
     ]
    }
   ],
   "source": [
    "if path.exists(\"subreddit_data\"):\n",
    "    print(\"Directory subreddit_data exists!\")\n",
    "else :\n",
    "    !mkdir subreddit_data\n",
    "    print(\"Directory subreddit_data created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Crawling with PRAW: a python wrapper for the Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T04:33:20.383301Z",
     "start_time": "2019-09-17T04:33:19.267578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "uname = \"DavidWithAnE\"\n",
    "upassword = \"BROCCO.billy85\"\n",
    "uagent = \"controversit by /u/\"+uname\n",
    "cl_id = \"l_eEWQRL2Qrvxw\"\n",
    "cl_secret = \"QBAlJLF4KPd4ox5ykGTsAoM9Z_A\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=cl_id,\n",
    "                     client_secret=cl_secret,\n",
    "                     password=upassword,\n",
    "                     user_agent=uagent,\n",
    "                     username=uname)\n",
    "\n",
    "print(\"Logged in as user: \",reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Choose a subreddit and get a list of submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T00:40:22.700812Z",
     "start_time": "2019-09-13T00:40:22.688846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_pd_dframe(nrows,list_of_fields):\n",
    "    '''\n",
    "    A simple function to create an empty\n",
    "    dataframe with a given number of rows \n",
    "    and a given list of columns\n",
    "    '''\n",
    "    \n",
    "    shape = (nrows,len(list_of_fields))\n",
    "    d = np.empty(shape)\n",
    "    df = pd.DataFrame(data=d,columns=list_of_fields)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def submission_data_to_dframe(sorting_scheme,n_subm=100):\n",
    "    '''\n",
    "    A function that gets n_subm reddit submissions, \n",
    "    retrieves their properties, comments to the \n",
    "    submission, then saves the results into a pandas \n",
    "    dataframe.\n",
    "    '''\n",
    "\n",
    "    # Initialize pandas dataframe\n",
    "    list_of_fields = [\"subm_ID\",\"subm_title\",\"subm_author\", \\\n",
    "                      \"subm_created_utc\",\"subm_upvote_ratio\", \\\n",
    "                      \"subm_link_flair_text\", \\\n",
    "                      \"subm_comment_ids\", \\\n",
    "                      \"subm_comment_authors\", \\\n",
    "                      \"subm_comment_bodies\", \\\n",
    "                      \"subm_comment_scores\"]\n",
    "    df = create_pd_dframe(n_subm,list_of_fields)\n",
    "    df[\"subm_comment_ids\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_authors\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_bodies\"] = [[]] * len(df)\n",
    "    df[\"subm_comment_scores\"] = [[]] * len(df)\n",
    "        \n",
    "    # Use reddit api to retrieve the necessary information.\n",
    "    i = 0 \n",
    "    for sub in sorting_scheme(limit=n_subm):\n",
    "\n",
    "        # Data on submissions\n",
    "        df[\"subm_ID\"].iloc[i] = sub.id\n",
    "        df[\"subm_title\"].iloc[i] = sub.title\n",
    "        df[\"subm_author\"].iloc[i] = sub.author\n",
    "        df[\"subm_created_utc\"].iloc[i] = float(sub.created_utc)\n",
    "        df[\"subm_upvote_ratio\"].iloc[i] = float(sub.upvote_ratio)\n",
    "        df[\"subm_link_flair_text\"].iloc[i] = sub.link_flair_text\n",
    "        \n",
    "        # Data on comments\n",
    "        sub.comments.replace_more(limit=0)\n",
    "        comment_ids = []\n",
    "        comment_authors = []\n",
    "        comment_bodies = []\n",
    "        comment_scores = []\n",
    "        for com in sub.comments:\n",
    "            comment_ids.append(com.id)\n",
    "            comment_authors.append(com.author)\n",
    "            comment_bodies.append(com.body)\n",
    "            comment_scores.append(float(com.score))\n",
    "        df[\"subm_comment_ids\"].iloc[i] = comment_ids\n",
    "        df[\"subm_comment_authors\"].iloc[i] = comment_authors\n",
    "        df[\"subm_comment_bodies\"].iloc[i] = comment_bodies\n",
    "        df[\"subm_comment_scores\"].iloc[i] = comment_scores\n",
    "        \n",
    "        # Increase counter\n",
    "        i=i+1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:46:46.464469Z",
     "start_time": "2019-09-13T01:43:34.574765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose a subreddit and a number of submissions\n",
    "\n",
    "subreddit_name = \"politicaldiscussion\"\n",
    "#subreddit_name = \"amitheasshole\"\n",
    "#subreddit_name = \"changemyview\"\n",
    "\n",
    "n_subm = 1000\n",
    "\n",
    "subreddit_obj = reddit.subreddit(subreddit_name)\n",
    "\n",
    "subm_data = submission_data_to_dframe(subreddit_obj.new,n_subm=n_subm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:47:04.112251Z",
     "start_time": "2019-09-13T01:47:04.086925Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display dataframe\n",
    "subm_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dump data into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:47:10.204254Z",
     "start_time": "2019-09-13T01:47:10.189359Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "timestamp = time.time()\n",
    "dati = datetime.datetime.fromtimestamp(timestamp)\n",
    "da = str(dati)[0:10]\n",
    "ti = str(dati)[11:19]\n",
    "\n",
    "fname = \"dump_r-\"+subreddit_name+\"_\"+da+\".pkl\"\n",
    "print(fname)\n",
    "subm_data.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Make a few diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:39:53.102630Z",
     "start_time": "2019-09-13T06:39:52.582606Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make an histogram of the controversial indicator \n",
    "# i.e. subm_upvote_ratio\n",
    "\n",
    "# Load a couple of \n",
    "df0 = pd.read_pickle(\"dump_r-politicaldiscussion_2019-09-12.pkl\")\n",
    "name0 = \"politicaldiscussion\"\n",
    "df1 = pd.read_pickle(\"dump_r-amitheasshole_2019-09-12.pkl\")\n",
    "name1 = \"amitheasshole\"\n",
    "df2 = pd.read_pickle(\"dump_r-changemyview_2019-09-12.pkl\")\n",
    "name2 = \"changemyview\"\n",
    "\n",
    "params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.axis([-0.05,1.05,0.1,200])\n",
    "plt.xlabel(\"Upvotes/(Upvotes+Downvotes)\")\n",
    "plt.xticks(np.arange(0,1.2,0.2),labels=[\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1.0\"])\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "plt.yticks([0.1,1,10,100],labels=[\"0.1\",\"1.0\",\"10\",\"100\"])\n",
    "plt.axvspan(0.25, 0.75, facecolor='gray',alpha=0.3,label=\"Controversial Range\")\n",
    "plt.plot([0.5,0.5],[0,1e8],\"k:\",linewidth=5,label=\"Most Controversial\")\n",
    "histdata = [df0[\"subm_upvote_ratio\"].values,df1[\"subm_upvote_ratio\"].values,df2[\"subm_upvote_ratio\"].values]\n",
    "\n",
    "colors = [\"red\",\"lightgreen\",\"lightblue\"]\n",
    "labels = [\"r/\"+name0,\"r/\"+name1,\"r/\"+name2]\n",
    "plt.hist(histdata,bins=10,range=[0,1],rwidth=0.8,label=labels,color=colors,edgecolor='black',alpha=1)\n",
    "plt.legend(loc='top left', bbox_to_anchor=(1.05, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:39:54.985069Z",
     "start_time": "2019-09-13T06:39:54.717219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Another interesting plot\n",
    "\n",
    "# Load a couple of \n",
    "df0 = pd.read_pickle(\"dump_r-politicaldiscussion_2019-09-12.pkl\")\n",
    "name0 = \"politicaldiscussion\"\n",
    "df1 = pd.read_pickle(\"dump_r-amitheasshole_2019-09-12.pkl\")\n",
    "name1 = \"amitheasshole\"\n",
    "df2 = pd.read_pickle(\"dump_r-changemyview_2019-09-12.pkl\")\n",
    "name2 = \"changemyview\"\n",
    "\n",
    "params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "sel0 = df0[\"subm_upvote_ratio\"] <= 0.25\n",
    "sel1 = df1[\"subm_upvote_ratio\"] <= 0.25\n",
    "sel2 = df2[\"subm_upvote_ratio\"] <= 0.25\n",
    "negative_opinion = np.array([ \\\n",
    "                    float(len(df0[\"subm_upvote_ratio\"][sel0]))/float(len(df0[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df1[\"subm_upvote_ratio\"][sel1]))/float(len(df1[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df2[\"subm_upvote_ratio\"][sel0]))/float(len(df2[\"subm_upvote_ratio\"])) \\\n",
    "                   ])\n",
    "\n",
    "sel0 = df0[\"subm_upvote_ratio\"] >= 0.75\n",
    "sel1 = df1[\"subm_upvote_ratio\"] >= 0.75\n",
    "sel2 = df2[\"subm_upvote_ratio\"] >= 0.75\n",
    "positive_opinion = np.array([ \\\n",
    "                    float(len(df0[\"subm_upvote_ratio\"][sel0]))/float(len(df0[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df1[\"subm_upvote_ratio\"][sel1]))/float(len(df1[\"subm_upvote_ratio\"])), \\\n",
    "                    float(len(df2[\"subm_upvote_ratio\"][sel0]))/float(len(df2[\"subm_upvote_ratio\"])) \\\n",
    "                   ])\n",
    "\n",
    "controversial_opinion = np.array([1.0,1.0,1.0])\n",
    "controversial_opinion = controversial_opinion-positive_opinion-negative_opinion\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ind = [0,1,2]\n",
    "p1 = plt.barh(ind,negative_opinion,height=0.5,color=\"red\",label=\"Negative\")\n",
    "p2 = plt.barh(ind,controversial_opinion,height=0.5,color=\"orange\",left=negative_opinion,label=\"Controversial\")\n",
    "p3 = plt.barh(ind,positive_opinion,color=\"green\",height=0.5,\\\n",
    "     left=negative_opinion+controversial_opinion, label=\"Positive\")\n",
    "plt.xlabel(\"Fraction of Submissions\")\n",
    "plt.yticks(ind,[\"r/\"+name0,\"r/\"+name1,\"r/\"+name2])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## A few additional tests with PRAW: run a search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T21:11:40.613731Z",
     "start_time": "2019-09-16T21:11:34.764921Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A test with a different API call\n",
    "\n",
    "#subreddit_name = \"politicaldiscussion\"\n",
    "#subreddit_name = \"amitheasshole\"\n",
    "subreddit_name = \"changemyview\"\n",
    "\n",
    "n_subm = 1000\n",
    "\n",
    "subreddit_obj = reddit.subreddit(subreddit_name)\n",
    "\n",
    "i=0\n",
    "for sub in subreddit_obj.search(\"all\", sort='new', syntax='lucene', time_filter='all', limit=10):\n",
    "    print(i,sub.title)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling with Pushshift.io\n",
    "\n",
    "More versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A series of functions to use the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:08.546179Z",
     "start_time": "2019-09-22T08:07:08.541946Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPushshiftData(after, before, limit, subr):\n",
    "    '''\n",
    "    This function queries submissions from a subreddit\n",
    "    within a given time range [before,after] using \n",
    "    a Pushshift.io call.\n",
    "    '''\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/?size=\"+ \\\n",
    "        str(limit)+\"&after=\"+str(after)+\"&before=\"+str(before)+\"&subreddit=\"+str(subr)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:09.427444Z",
     "start_time": "2019-09-22T08:07:09.422707Z"
    }
   },
   "outputs": [],
   "source": [
    "def getExtra(sub_id,useragent):\n",
    "    '''\n",
    "    Given a submission sub_id, get extra data\n",
    "    that is not provided by Pushshift.io, e.g.\n",
    "    score and upvote ratio.\n",
    "    '''\n",
    "    searchURL = 'http://reddit.com/'\n",
    "    url = searchURL + str(sub_id) + '.json'\n",
    "    r = requests.get(url, headers = {'User-agent': useragent})\n",
    "    extra_data = json.loads(r.text)\n",
    "    out0 = extra_data[0][\"data\"][\"children\"][0][\"data\"][\"score\"]\n",
    "    out1 = extra_data[0][\"data\"][\"children\"][0][\"data\"][\"upvote_ratio\"] \n",
    "    return out0,out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:10.398130Z",
     "start_time": "2019-09-22T08:07:10.389839Z"
    }
   },
   "outputs": [],
   "source": [
    "def collectSubData(subDic,subm,subr,useragent):\n",
    "    '''\n",
    "    This function collects data on submissions to a \n",
    "    given subreddit and saves them in a dictionary\n",
    "    '''\n",
    "    subDic[\"subreddit\"].append(subr)\n",
    "    subDic[\"sub_id\"].append(subm[\"id\"])\n",
    "    subDic[\"title\"].append(subm[\"title\"])\n",
    "    subDic[\"author\"].append(subm[\"author\"])\n",
    "    dt = str(datetime.datetime.fromtimestamp(subm[\"created_utc\"]))\n",
    "    subDic[\"created\"].append(dt)\n",
    "    subDic[\"url\"].append(subm[\"url\"])\n",
    "    subDic[\"permalink\"].append(subm[\"permalink\"])\n",
    "    try:\n",
    "        subDic[\"flair\"].append(subm[\"link_flair_text\"])\n",
    "    except KeyError:\n",
    "        subDic[\"flair\"].append(\"NaN\")\n",
    "    subDic[\"numComms\"].append(subm[\"num_comments\"])\n",
    "    try:\n",
    "        subDic[\"selftext\"].append(subm[\"selftext\"])\n",
    "    except KeyError:    \n",
    "        subDic[\"selftext\"].append(\"NaN\")\n",
    "    try:\n",
    "        score,upvote_ratio = getExtra(subm[\"id\"],useragent)\n",
    "        subDic[\"upvote_ratio\"].append(upvote_ratio)\n",
    "        subDic[\"score\"].append(score)\n",
    "    except:\n",
    "        subDic[\"upvote_ratio\"].append(\"Nan\")\n",
    "        subDic[\"score\"].append(subm[\"score\"])\n",
    "\n",
    "    return subDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:11.506716Z",
     "start_time": "2019-09-22T08:07:11.501768Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def runQuery(subCount, subDic,after, before, limit, subr, useragent):\n",
    "    '''\n",
    "    This function calls the appropriate functions to \n",
    "    run a Pushshift.io query to get submissions in \n",
    "    a given subreddit, then calls saveQueryData to \n",
    "    save data to disk in pickle format using pandas.\n",
    "    '''\n",
    "    # Run query\n",
    "    data = getPushshiftData(after, before, limit, subr)\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            subDic = collectSubData(subDic,submission,subr,useragent)\n",
    "            subCount+=1\n",
    "        after = data[-1][\"created_utc\"]\n",
    "        data = getPushshiftData(after, before, limit, subr)\n",
    "    \n",
    "    df = pd.DataFrame(data=subDic)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:12.648505Z",
     "start_time": "2019-09-22T08:07:12.645556Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveDFrame(df,subr):\n",
    "    outpath = \"./subreddit_data/\"\n",
    "    fname = outpath+\"r-\"+subr+\"-export.pkl\"\n",
    "    df.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T08:07:13.699281Z",
     "start_time": "2019-09-22T08:07:13.694219Z"
    }
   },
   "outputs": [],
   "source": [
    "def MPQueryWrapper(after,before,limit,subr,useragent):\n",
    "    '''\n",
    "    This functions wrap API calls for use with the \n",
    "    multiprocessing package. With this function \n",
    "    multiple queries can be run in parallel. \n",
    "    '''\n",
    "    # Fields that will be used\n",
    "    fields = [\"subreddit\",\"sub_id\",\"title\",\"author\", \\\n",
    "          \"created\",\"url\",\"permalink\",\"score\", \\\n",
    "          \"numComms\",\"flair\",\"selftext\",\"upvote_ratio\"]\n",
    "    subCount = 0\n",
    "    subDic = {}\n",
    "    for f in fields:\n",
    "        subDic[f] = [] \n",
    "    print(after,before,limit,subr,useragent)\n",
    "    df = runQuery(subCount,subDic,after,before,limit,subr,useragent)\n",
    "    \n",
    "    saveDFrame(df,subr)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose list of subreddits, a time range and a list of query parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T15:55:05.988409Z",
     "start_time": "2019-09-22T15:55:05.982982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useragent\n",
    "useragent = \"DavidWithAnE\"\n",
    "\n",
    "# Subreddit selection\n",
    "subr_list = [\"comsci\",\"politicaldiscussion\", \\\n",
    "             \"quotes\",\"history\"]\n",
    "\n",
    "# Max number of submissions per query\n",
    "limit = 10000\n",
    "\n",
    "# Time selection based on timestamps\n",
    "# This currently selects all posts\n",
    "#dateini = \"2019-01-01\" # Fiducial for advice, changemyview, confession, parenting\n",
    "dateini = \"2010-01-01\"\n",
    "dateend = \"2020-01-01\"\n",
    "tsi = ciso8601.parse_datetime(dateini)\n",
    "tsf = ciso8601.parse_datetime(dateend)\n",
    "# to get time in seconds:\n",
    "after = int(time.mktime(tsi.timetuple()))\n",
    "before = int(time.mktime(tsf.timetuple()))\n",
    "\n",
    "query_params = []\n",
    "for i in range(0,len(subr_list)):\n",
    "    query_params.append((after,before,limit,subr_list[i],useragent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and run queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-22T15:55:09.221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262332800 1577865600 10000 comsci DavidWithAnE\n",
      "1262332800 1577865600 10000 politicaldiscussion DavidWithAnE\n",
      "1262332800 1577865600 10000 quotes DavidWithAnE\n",
      "1262332800 1577865600 10000 history DavidWithAnE\n"
     ]
    }
   ],
   "source": [
    "# Run on multiple cores\n",
    "from itertools import starmap\n",
    "import multiprocessing as mp\n",
    "\n",
    "ncores = min(mp.cpu_count(),len(subr_list))\n",
    "worker_pool = mp.Pool(ncores)\n",
    "\n",
    "out = []\n",
    "for df in worker_pool.starmap(MPQueryWrapper, query_params):\n",
    "    out.append(df)\n",
    "        \n",
    "#Close threads\n",
    "worker_pool.close()\n",
    "worker_pool.join()\n",
    "\n",
    "display(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Alternative setup for very large subreddits: split time range in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T05:30:26.410378Z",
     "start_time": "2019-09-19T05:30:26.404223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Useragent\n",
    "useragent = \"DavidWithAnE\"\n",
    "\n",
    "# Subreddit selection\n",
    "subrname = \"quotes\"\n",
    "\n",
    "# Max number of submissions per query\n",
    "limit = 10000\n",
    "\n",
    "# Time selection based on timestamps\n",
    "# This currently selects all posts\n",
    "dateini = \"2019-01-01\"\n",
    "dateend = \"2020-09-30\"\n",
    "tsi = ciso8601.parse_datetime(dateini)\n",
    "tsf = ciso8601.parse_datetime(dateend)\n",
    "# to get time in seconds:\n",
    "dateini = int(time.mktime(tsi.timetuple()))\n",
    "dateend = int(time.mktime(tsf.timetuple()))\n",
    "\n",
    "nchunks = 8\n",
    "dt = int((dateend-dateini)/nchunks)\n",
    "query_params = []\n",
    "for i in range(0,nchunks):\n",
    "    after = dateini+int(i*dt)\n",
    "    if i == nchunks-1 :\n",
    "        before = dateend\n",
    "    else :\n",
    "        before = dateini+int((i+1)*dt)\n",
    "    query_params.append((after,before,limit,subrname,useragent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run API query on the time chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T06:08:08.718175Z",
     "start_time": "2019-09-19T05:30:28.381776Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546329600 1553219550 10000 quotes DavidWithAnE\n",
      "1553219550 1560109500 10000 quotes DavidWithAnE\n",
      "1560109500 1566999450 10000 quotes DavidWithAnE\n",
      "1566999450 1573889400 10000 quotes DavidWithAnE\n",
      "1573889400 1580779350 10000 quotes DavidWithAnE\n",
      "1587669300 1594559250 10000 quotes DavidWithAnE\n",
      "1580779350 1587669300 10000 quotes DavidWithAnE\n",
      "1594559250 1601449200 10000 quotes DavidWithAnE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-71:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-69:\n",
      "Process ForkPoolWorker-67:\n",
      "Process ForkPoolWorker-66:\n",
      "Process ForkPoolWorker-70:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9580a04ac2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworker_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMPQueryWrapperTimeChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         '''\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/base-py3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/base-py3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-64:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dmartizzi/anaconda2/envs/base-py3/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run on multiple cores\n",
    "from itertools import starmap\n",
    "import multiprocessing as mp\n",
    "\n",
    "ncores = min(mp.cpu_count(),nchunks)\n",
    "worker_pool = mp.Pool(ncores)\n",
    "\n",
    "worker_pool.starmap(MPQueryWrapper, query_params):\n",
    "        \n",
    "#Close threads\n",
    "worker_pool.close()\n",
    "worker_pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a new SQL database for the data \n",
    "\n",
    "These are prototypes and need to be coded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:00:15.195302Z",
     "start_time": "2019-09-16T18:00:11.332644Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:00:15.200296Z",
     "start_time": "2019-09-16T18:00:15.197453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a database name \n",
    "# Set your postgres username\n",
    "dbname = 'subreddit-db'\n",
    "username = 'dmartizzi' # change this to your username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T16:58:47.049710Z",
     "start_time": "2019-09-18T16:58:44.419Z"
    }
   },
   "outputs": [],
   "source": [
    "subr_list_to_sql = [\"politicaldiscussion\", \"history\",\"changemyview\",\"quotes\"]\n",
    "for sss in subr_list_to_sql:\n",
    "    fname = \"r-\"+sss+\"-export.pkl\"\n",
    "    subr_data = pd.read_pickle(fname)\n",
    "    subr_data.to_sql(s, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
