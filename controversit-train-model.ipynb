{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a NLP model for the topics in a given subreddit then train a regression model to predict the popularity and controversiality of a post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:12:32.197868Z",
     "start_time": "2019-09-30T17:11:41.229050Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.test.utils import datapath\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from textblob import TextBlob\n",
    "import sklearn\n",
    "import sklearn.model_selection as skmodsel\n",
    "import sklearn.linear_model as sklinmod\n",
    "import sklearn.ensemble as skensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: Load data from a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:12:33.191798Z",
     "start_time": "2019-09-30T17:12:32.199864Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#subrname = \"politicaldiscussion\"\n",
    "#subrname = \"history\"\n",
    "#subrname = \"quotes\"\n",
    "#subrname = \"compsci\"\n",
    "subrname = \"changemyview\"\n",
    "#subrname = \"parenting\"\n",
    "\n",
    "#inpath = \"./\"\n",
    "#fname = \"dump_r-\"+subrname+\"_2019-09-12.pkl\"\n",
    "inpath = \"./subreddit_data/\"\n",
    "fname = inpath+\"r-\"+subrname+\"-export.pkl\"\n",
    "dfraw = pd.read_pickle(fname)\n",
    "\n",
    "# Filter out removed posts\n",
    "sel = dfraw[\"selftext\"].str.strip() == \"[removed]\"\n",
    "dfraw[\"selftext\"][sel] = \"\"\n",
    "dfraw[\"title+selftext\"] = dfraw[\"title\"]+\" \"+dfraw[\"selftext\"]\n",
    "dfraw[\"Popularity\"] = dfraw[\"score\"]+dfraw[\"numComms\"]\n",
    "dfraw[\"Controversiality\"] = 1.0-np.abs((dfraw[\"upvote_ratio\"].values.astype(float)-0.5)/0.5)\n",
    "\n",
    "# Filter out posts with less than 3 words\n",
    "n_w = [len(dfraw[\"title+selftext\"][i].split()) for i in range(0,len(dfraw[\"title+selftext\"]))]\n",
    "dfraw[\"n_words\"] = n_w\n",
    "sel = dfraw[\"n_words\"] > 3\n",
    "df = dfraw[sel]    \n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plots for the the given subreddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:50:52.178919Z",
     "start_time": "2019-09-26T20:50:51.366647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "xmin = -0.3\n",
    "xmedian = np.quantile(np.log10(df[\"Popularity\"].values),0.5)\n",
    "xmax = np.log10(np.max(df[\"Popularity\"].values))\n",
    "ymin = 1.01\n",
    "isel = df[\"Popularity\"].values == 1\n",
    "ymax = 5*len(df[\"Popularity\"].values[isel])\n",
    "plt.axis([xmin,1.05*xmax,ymin,ymax])\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "plt.xticks(np.arange(0,int(1.05*xmax)+1),labels=[\"%d\"%(10**i) for i in np.arange(0,int(1.05*xmax)+1)])\n",
    "plt.axvspan(-1, xmedian, facecolor='gray',alpha=0.5,label=\"Unpopular Range\")\n",
    "plt.hist(np.log10(df[\"Popularity\"].values),bins=int((xmax-xmin)*5),range=[xmin,xmax],rwidth=0.8,color='lightblue',edgecolor='black',alpha=1,label=\"r/\"+subrname)\n",
    "plt.plot([xmedian,xmedian],[ymin,ymax],'r-',label=\"Popularity Threshold\")\n",
    "plt.title(\"Popularity: # Comments + # Upvotes\")\n",
    "plt.legend(loc=1)\n",
    "\n",
    "figoutpath=\"./pre_rendered_plots\"\n",
    "plt.savefig(figoutpath+\"/popularity_hist_r-\"+subrname+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:50:52.871038Z",
     "start_time": "2019-09-26T20:50:52.266952Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Controversiality\")\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "ymin = 1.01\n",
    "isel = df[\"Controversiality\"].values < 0.2\n",
    "ymax = 5.*len(df[\"Controversiality\"].values[isel])\n",
    "plt.axis([-0.01,1.0,ymin,ymax])\n",
    "plt.xticks(np.arange(0,1.2,0.2),labels=[\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1.0\"])\n",
    "plt.axvspan(0.5, 1.0, facecolor='gray',alpha=0.5,label=\"Controversial Range\")\n",
    "plt.plot([0.5,0.5],[0,1e8],\"r-\",linewidth=5,label=\"Controversiality Threshold\")\n",
    "histdata = [df[\"Controversiality\"].values]\n",
    "plt.hist(histdata,bins=20,range=[0,1],rwidth=0.8,color='lightblue',edgecolor='black',alpha=1,label=\"r/\"+subrname)\n",
    "plt.title(\"Controversiality = 1-|2*#Upvotes/(#Upvotes+#Downvotes)-1|\")\n",
    "plt.legend(loc=1)\n",
    "figoutpath=\"./pre_rendered_plots\"\n",
    "plt.savefig(figoutpath+\"/popularity_hist_r-\"+subrname+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the submissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:12:33.196908Z",
     "start_time": "2019-09-30T17:12:33.193867Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''Function to lemmatize text'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:12:33.202065Z",
     "start_time": "2019-09-30T17:12:33.198679Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''Function to pre-process text'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:27.244568Z",
     "start_time": "2019-09-30T17:12:33.203921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess titles \n",
    "processed_subm = df[\"title+selftext\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experimental: load LDA model or create it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T19:01:22.479332Z",
     "start_time": "2019-09-26T19:01:15.930591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Bag of Words \n",
    "dictionary = gensim.corpora.Dictionary(processed_subm)\n",
    "\n",
    "# Filter out irrelevant words \n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=5000)\n",
    "\n",
    "# For each tweet, create a dictionary reporting how many\n",
    "# words and how many times those words appear.\n",
    "bow_corpus = [dictionary.doc2bow(subm) for subm in processed_subm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:05:58.357211Z",
     "start_time": "2019-09-21T21:05:58.350500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if path.exists(\"lda_models\"):\n",
    "    print(\"Directory lda_models exists!\")\n",
    "else:\n",
    "    !mkdir lda_models\n",
    "    print(\"Directory lda_models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:05:58.363418Z",
     "start_time": "2019-09-21T21:05:58.360200Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose number of topics for LDA\n",
    "num_topics_lda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:05:58.513832Z",
     "start_time": "2019-09-21T21:05:58.370566Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outpath = \"./lda_models/\"\n",
    "ldaoutfile = outpath+\"lda_model_prod_r-\"+subrname\n",
    "if path.exists(ldaoutfile):\n",
    "    print(ldaoutfile+\"model exists!\")\n",
    "    lda_model = gensim.models.LdaModel.load(ldaoutfile)\n",
    "else :\n",
    "    # Create Latent Dirichlet Allocation model with a given number of topics\n",
    "    ncores = mp.cpu_count()\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics_lda, id2word=dictionary, \\\n",
    "                passes=10, workers=ncores)\n",
    "    # Save LDA model to disc (it's expensive to regenerate)\n",
    "    lda_model.save(ldaoutfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Perform exploratory data analysis with the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:05:58.526531Z",
     "start_time": "2019-09-21T21:05:58.519912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:05:58.565761Z",
     "start_time": "2019-09-21T21:05:58.529918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda_topics_df = get_lda_topics(lda_model, num_topics_lda)\n",
    "lda_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:07:37.869710Z",
     "start_time": "2019-09-21T21:05:58.568146Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test LDA model\n",
    "test_strings = {\"test_strings\" : list(df[\"title+selftext\"].values)}\n",
    "df_test_strings = pd.DataFrame(data = test_strings)\n",
    "\n",
    "# Same procedure as above\n",
    "processed_test_strings = df_test_strings[\"test_strings\"].map(preprocess)\n",
    "test_corpus = [dictionary.doc2bow(text) for text in processed_test_strings]\n",
    "raw = lda_model[test_corpus]\n",
    "shape = (len(raw),num_topics_lda)\n",
    "predicted_topic_lda = np.zeros(shape)\n",
    "for i,tups in enumerate(raw):\n",
    "    for j in range(0,len(tups)):\n",
    "        predicted_topic_lda[i,j] = tups[j][1]\n",
    "    #print(df_test_strings[\"test_strings\"].iloc[i],predicted_topic_lda[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: load NMF model or create it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:27.250852Z",
     "start_time": "2019-09-30T17:13:27.246523Z"
    }
   },
   "outputs": [],
   "source": [
    "if path.exists(\"nmf_models\"):\n",
    "    print(\"Directory nmf_models exists!\")\n",
    "else:\n",
    "    !mkdir nmf_models\n",
    "    print(\"Directory nmf_models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:27.255051Z",
     "start_time": "2019-09-30T17:13:27.252891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of topics for NMF\n",
    "num_topics_nmf = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:28.531612Z",
     "start_time": "2019-09-30T17:13:27.257063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create sentences\n",
    "processed_subm_sentences = [' '.join(text) for text in processed_subm]\n",
    "\n",
    "# Word counts\n",
    "outpath = \"./nmf_models/\"\n",
    "outfile = outpath+\"count_vectorizer_prod_r-\"+subrname\n",
    "if path.exists(outfile):\n",
    "    print(outfile+\"model exists!\")\n",
    "    vectorizer = pickle.load(open(outfile, 'rb'))    \n",
    "    x_counts = vectorizer.transform(processed_subm_sentences)\n",
    "else:\n",
    "    vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "    x_counts = vectorizer.fit_transform(processed_subm_sentences)\n",
    "    pickle.dump(vectorizer, open(outfile, 'wb'))\n",
    "    \n",
    "# TF-IDF transform\n",
    "outpath = \"./nmf_models/\"\n",
    "outfile = outpath+\"tfidf_transformer_prod_r-\"+subrname\n",
    "if path.exists(outfile):\n",
    "    print(outfile+\"model exists!\")\n",
    "    transformer = pickle.load(open(outfile, 'rb')) \n",
    "    x_tfidf = transformer.transform(x_counts)\n",
    "else:\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    x_tfidf = transformer.fit_transform(x_counts)\n",
    "    pickle.dump(transformer, open(outfile, 'wb'))\n",
    "\n",
    "# Normalize to unit length\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:28.546032Z",
     "start_time": "2019-09-30T17:13:28.534257Z"
    }
   },
   "outputs": [],
   "source": [
    "outpath = \"./nmf_models/\"\n",
    "nmfoutfile = outpath+\"nmf_model_prod_r-\"+subrname\n",
    "if path.exists(nmfoutfile):\n",
    "    print(nmfoutfile+\"model exists!\")\n",
    "    nmf_model = pickle.load(open(nmfoutfile, 'rb'))\n",
    "else :\n",
    "    # Create NMF model.\n",
    "    nmf_model = NMF(n_components=num_topics_nmf,init='nndsvd',alpha=0.1)\n",
    "    # Fit the model\n",
    "    nmf_model.fit(xtfidf_norm)\n",
    "    # Save NMF model to disc (it's expensive to regenerate)\n",
    "    pickle.dump(nmf_model, open(nmfoutfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:28.553905Z",
     "start_time": "2019-09-30T17:13:28.548312Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words, num_topics):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:13:28.604140Z",
     "start_time": "2019-09-30T17:13:28.555944Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_topics_df = get_nmf_topics(nmf_model, 10, num_topics_nmf)\n",
    "nmf_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:14:23.255287Z",
     "start_time": "2019-09-30T17:13:28.606157Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Test NMF model\n",
    "test_strings = {\"test_strings\" : df[\"title+selftext\"].values}\n",
    "df_test_strings = pd.DataFrame(data = test_strings)\n",
    "\n",
    "# Same procedure as above\n",
    "processed_test_strings = df_test_strings[\"test_strings\"].map(preprocess)\n",
    "test_sentences = [' '.join(text) for text in processed_test_strings]\n",
    "x_test_counts = vectorizer.transform(test_sentences)\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\n",
    "xtfidf_test_norm = normalize(x_test_tfidf, norm='l1', axis=1)\n",
    "y = nmf_model.transform(xtfidf_test_norm)\n",
    "predicted_topic_nmf = normalize(y, norm='l1', axis=1)\n",
    "#for i in range(0,len(predicted_topic_nmf)):\n",
    "#    print(df_test_strings[\"test_strings\"].iloc[i],predicted_topic_nmf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:51:56.090573Z",
     "start_time": "2019-09-26T20:51:49.812629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentiment = np.empty((predicted_topic_nmf[:,0].shape[0],2))\n",
    "for i,sentence in enumerate(processed_subm_sentences):\n",
    "    proc = TextBlob(sentence)\n",
    "    sentiment[i,0] = proc.sentiment[0]\n",
    "    sentiment[i,1] = proc.sentiment[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: perform regression only with topics to predict popularity and controversiality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Popularity and Controversiality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:26:55.894232Z",
     "start_time": "2019-09-30T17:26:55.880853Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build labels and features\n",
    "\n",
    "features = predicted_topic_nmf\n",
    "\n",
    "# Popularity of a post\n",
    "p = [0] * len(df[\"Popularity\"].values)\n",
    "popular = np.array(p); del p\n",
    "cutoff_pop = np.quantile(df[\"Popularity\"].values,q=0.5)\n",
    "selp0 = df[\"Popularity\"].values > cutoff_pop\n",
    "popular[selp0] = 1\n",
    "popular = popular.astype(int)\n",
    "\n",
    "# Controversiality of a post\n",
    "c = [0] * len(df[\"upvote_ratio\"].values)\n",
    "controversial = np.array(c); del c\n",
    "inf = 0.25\n",
    "sup = 0.75\n",
    "selc0 = np.logical_and(inf <= df[\"upvote_ratio\"].values.astype(float), \\\n",
    "                       df[\"upvote_ratio\"].values.astype(float) <= sup)\n",
    "controversial[selc0] = 1\n",
    "selc1 = np.logical_or(df[\"upvote_ratio\"].values.astype(float) < inf, \\\n",
    "                      df[\"upvote_ratio\"].values.astype(float) > sup)\n",
    "controversial = controversial.astype(int)\n",
    "\n",
    "Fpop = float(len(popular[selp0])/len(popular))\n",
    "Fcon = float(len(controversial[selc0])/len(controversial))\n",
    "\n",
    "print(\"Fraction of Popular Posts = \",float(len(popular[selp0])/len(popular)))\n",
    "print(\"Fraction of Controversial Posts = \",float(len(controversial[selc0])/len(controversial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T22:18:40.058436Z",
     "start_time": "2019-09-17T22:18:40.055549Z"
    }
   },
   "source": [
    "## Split in training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:26:59.264354Z",
     "start_time": "2019-09-30T17:26:59.213418Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "train_size = 1.0 - test_size - validation_size\n",
    "\n",
    "X, featurep_test, Y, popular_test = \\\n",
    "    skmodsel.train_test_split(features,popular,test_size=test_size,stratify=popular)\n",
    "featurep_train, featurep_validation, popular_train, popular_validation = \\\n",
    "    skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "\n",
    "X, featurec_test, Y, controversial_test = \\\n",
    "    skmodsel.train_test_split(features,controversial,test_size=test_size,stratify=controversial)\n",
    "featurec_train, featurec_validation, controversial_train, controversial_validation = \\\n",
    "    skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train Logistic Regression models\n",
    "\n",
    "These models are robust but do not achieve top precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:09:02.896360Z",
     "start_time": "2019-09-21T21:09:02.716090Z"
    }
   },
   "outputs": [],
   "source": [
    "wp = [1.0] * featurep_train.shape[0]\n",
    "weight_pop = np.array(wp); del wp\n",
    "selp0 = weight_pop == 0\n",
    "weight_pop[selp0] = Fpop\n",
    "#weight_pop = {0: Fpop, 1: 1.0}\n",
    "\n",
    "wc = [1.0] * featurec_train.shape[0]\n",
    "weight_con = np.array(wc); del wc\n",
    "selc1 = weight_con == 0\n",
    "weight_con[selc1] = Fcon\n",
    "#weight_pop = {0: Fcon, 1: 1.0}\n",
    "\n",
    "popular_logregmodel = sklinmod.LogisticRegression()\n",
    "controversial_logregmodel = sklinmod.LogisticRegression()\n",
    "popular_logregmodel = popular_logregmodel.fit(featurep_train,popular_train, \\\n",
    "                            sample_weight=weight_pop)\n",
    "controversial_logregmodel = controversial_logregmodel.fit(featurec_train,controversial_train, \\\n",
    "                            sample_weight=weight_con)\n",
    "\n",
    "pred_pop_train = popular_logregmodel.predict(featurep_train)\n",
    "pred_pop_test = popular_logregmodel.predict(featurep_test)\n",
    "pred_pop_val = popular_logregmodel.predict(featurep_validation)\n",
    "pred_con_train = popular_logregmodel.predict(featurec_train)\n",
    "pred_con_test = popular_logregmodel.predict(featurec_test)\n",
    "pred_con_val = popular_logregmodel.predict(featurec_validation)\n",
    "\n",
    "prec_pop_train = sklearn.metrics.precision_score(popular_train,pred_pop_train)\n",
    "prec_pop_test = sklearn.metrics.precision_score(popular_test,pred_pop_test)\n",
    "prec_pop_val = sklearn.metrics.precision_score(popular_validation,pred_pop_val)\n",
    "prec_con_train = sklearn.metrics.precision_score(controversial_train,pred_con_train)\n",
    "prec_con_test = sklearn.metrics.precision_score(controversial_test,pred_con_test)\n",
    "prec_con_val = sklearn.metrics.precision_score(controversial_validation,pred_con_val)\n",
    "\n",
    "print(\"Logistic regression for Popularity. Score on Training Set = \", prec_pop_train)\n",
    "print(\"Logistic regression for Popularity. Score on Test Set = \", prec_pop_test)\n",
    "print(\"Logistic regression for Popularity. Score on Validation Set = \", prec_pop_val,\"\\n\")\n",
    "\n",
    "print(\"Logistic regression for Controversiality. Score on Training Set = \", prec_con_train)\n",
    "print(\"Logistic regression for Controversiality. Score on Training Set = \", prec_con_test)\n",
    "print(\"Logistic regression for Controversiality. Score on Training Set = \", prec_con_val,\"\\n\")\n",
    "\n",
    "#outpath = \"./regression_models/\"\n",
    "#outname = outpath+\"lorgreg_scores_nmf_prod_r-\"+subrname+\".csv\"\n",
    "#with open(outname, 'w') as csvfile:\n",
    "#    writer = csv.writer(csvfile,delimiter=\",\")\n",
    "#    writer.writerow([\"PopTraniningScore\", \"PopTestScore\", \"PopValidationScore\", \\\n",
    "#                    \"ConTraniningScore\", \"ConTestScore\", \"ConValidationScore\", \n",
    "#                    \"PopFraction\",\"ConFraction\"])\n",
    "#    writer.writerow([str(prec_pop_train), \\\n",
    "#                    str(prec_pop_test), \\\n",
    "#                    str(prec_pop_val), \\\n",
    "#                    str(prec_con_train), \\\n",
    "#                    str(prec_con_test), \\\n",
    "#                    str(prec_con_val), \\\n",
    "#                    str(Fpop),str(Fcon)])\n",
    "#\n",
    "#scores = pd.read_csv(outname)\n",
    "#display(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:09:02.915447Z",
     "start_time": "2019-09-21T21:09:02.901273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save regression model to disc \n",
    "outpath = \"./regression_models/\"\n",
    "fname = outpath+\"logreg_popular_from_nmf_prod_r-\"+subrname\n",
    "pickle.dump(popular_logregmodel, open(fname, 'wb'))\n",
    "fname = outpath+\"logreg_controversial_from_nmf_prod_r-\"+subrname\n",
    "pickle.dump(controversial_logregmodel, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train Random Forests models\n",
    "\n",
    "These models seem to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T21:27:52.890411Z",
     "start_time": "2019-09-21T21:27:51.167468Z"
    }
   },
   "outputs": [],
   "source": [
    "wp = [1.0] * featurep_train.shape[0]\n",
    "weight_pop = np.array(wp); del wp\n",
    "selp0 = weight_pop == 0\n",
    "weight_pop[selp0] = Fpop\n",
    "#weight_pop = {0: Fpop, 1: 1.0}\n",
    "\n",
    "wc = [1.0] * featurec_train.shape[0]\n",
    "weight_con = np.array(wc); del wc\n",
    "selc1 = weight_con == 0\n",
    "weight_con[selc1] = Fcon\n",
    "#weight_pop = {0: Fcon, 1: 1.0}\n",
    "\n",
    "popular_rforestmodel = skensemble.RandomForestRegressor( \\\n",
    "                        n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "controversial_rforestmodel = skensemble.RandomForestRegressor( \\\n",
    "                        n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "\n",
    "popular_rforestmodel = popular_rforestmodel.fit(featurep_train,popular_train, \\\n",
    "                       sample_weight=weight_pop)\n",
    "controversial_rforestmodel = controversial_rforestmodel.fit(featurec_train,controversial_train, \\\n",
    "                             sample_weight=weight_con)\n",
    "\n",
    "pred_pop_train = np.rint(popular_rforestmodel.predict(featurep_train))\n",
    "pred_pop_test = np.rint(popular_rforestmodel.predict(featurep_test))\n",
    "pred_pop_val = np.rint(popular_rforestmodel.predict(featurep_validation))\n",
    "pred_con_train = np.rint(popular_rforestmodel.predict(featurec_train))\n",
    "pred_con_test = np.rint(popular_rforestmodel.predict(featurec_test))\n",
    "pred_con_val = np.rint(popular_rforestmodel.predict(featurec_validation))\n",
    "\n",
    "prec_pop_train = sklearn.metrics.precision_score(popular_train,pred_pop_train)\n",
    "prec_pop_test = sklearn.metrics.precision_score(popular_test,pred_pop_test)\n",
    "prec_pop_val = sklearn.metrics.precision_score(popular_validation,pred_pop_val)\n",
    "prec_con_train = sklearn.metrics.precision_score(controversial_train,pred_con_train)\n",
    "prec_con_test = sklearn.metrics.precision_score(controversial_test,pred_con_test)\n",
    "prec_con_val = sklearn.metrics.precision_score(controversial_validation,pred_con_val)\n",
    "\n",
    "print(\"Random Forest for Popularity. Score on Training Set = \", prec_pop_train)\n",
    "print(\"Random Forest for Popularity. Score on Test Set = \", prec_pop_test)\n",
    "print(\"Random Forest for Popularity. Score on Validation Set = \", prec_pop_val,\"\\n\")\n",
    "\n",
    "print(\"Random Forest for Controversiality. Score on Training Set = \", prec_con_train)\n",
    "print(\"Random Forest for Controversiality. Score on Training Set = \", prec_con_test)\n",
    "print(\"Random Forest for Controversiality. Score on Training Set = \", prec_con_val,\"\\n\")\n",
    "\n",
    "outpath = \"./regression_models/\"\n",
    "outname = outpath+\"rforest_scores_nmf_prod_r-\"+subrname+\".csv\"\n",
    "with open(outname, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile,delimiter=\",\")\n",
    "    writer.writerow([\"PopTraniningScore\", \"PopTestScore\", \"PopValidationScore\", \\\n",
    "                    \"ConTraniningScore\", \"ConTestScore\", \"ConValidationScore\", \n",
    "                    \"PopFraction\",\"ConFraction\"])\n",
    "    writer.writerow([str(prec_pop_train), \\\n",
    "                    str(prec_pop_test), \\\n",
    "                    str(prec_pop_val), \\\n",
    "                    str(prec_con_train), \\\n",
    "                    str(prec_con_test), \\\n",
    "                    str(prec_con_val), \\\n",
    "                    str(Fpop),str(Fcon)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train Gradient Boosting models\n",
    "\n",
    "These models are robust and achieve best precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:27:17.713299Z",
     "start_time": "2019-09-30T17:27:08.576564Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wp = [1.0] * featurep_train.shape[0]\n",
    "weight_pop = np.array(wp); del wp\n",
    "selp0 = weight_pop == 0\n",
    "weight_pop[selp0] = Fpop\n",
    "#weight_pop = {0: Fpop, 1: 1.0}\n",
    "\n",
    "wc = [1.0] * featurec_train.shape[0]\n",
    "weight_con = np.array(wc); del wc\n",
    "selc1 = weight_con == 0\n",
    "weight_con[selc1] = Fcon\n",
    "#weight_pop = {0: Fcon, 1: 1.0}\n",
    "\n",
    "popular_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                        n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "controversial_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                        n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "\n",
    "popular_gboost = popular_gboost.fit(featurep_train,popular_train, \\\n",
    "                       sample_weight=weight_pop)\n",
    "controversial_gboost = controversial_gboost.fit(featurec_train,controversial_train, \\\n",
    "                       sample_weight=weight_con)\n",
    "\n",
    "pred_pop_train = np.rint(popular_gboost.predict(featurep_train)).astype(int)\n",
    "pred_pop_test = np.rint(popular_gboost.predict(featurep_test)).astype(int)\n",
    "pred_pop_val = np.rint(popular_gboost.predict(featurep_validation)).astype(int)\n",
    "pred_con_train = np.rint(popular_gboost.predict(featurec_train)).astype(int)\n",
    "pred_con_test = np.rint(popular_gboost.predict(featurec_test)).astype(int)\n",
    "pred_con_val = np.rint(popular_gboost.predict(featurec_validation)).astype(int)\n",
    "\n",
    "prec_pop_train = sklearn.metrics.precision_score(popular_train,pred_pop_train)\n",
    "prec_pop_test = sklearn.metrics.precision_score(popular_test,pred_pop_test)\n",
    "prec_pop_val = sklearn.metrics.precision_score(popular_validation,pred_pop_val)\n",
    "prec_con_train = sklearn.metrics.precision_score(controversial_train,pred_con_train)\n",
    "prec_con_test = sklearn.metrics.precision_score(controversial_test,pred_con_test)\n",
    "prec_con_val = sklearn.metrics.precision_score(controversial_validation,pred_con_val)\n",
    "\n",
    "print(\"Gradient Boost for Popularity. Score on Training Set = \", prec_pop_train)\n",
    "print(\"Gradient Boost for Popularity. Score on Test Set = \", prec_pop_test)\n",
    "print(\"Gradient Boost for Popularity. Score on Validation Set = \", prec_pop_val,\"\\n\")\n",
    "\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_train)\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_test)\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_val,\"\\n\")\n",
    "\n",
    "#outpath = \"./regression_models/\"\n",
    "#outname = outpath+\"gboost_scores_nmf_prod_r-\"+subrname+\".csv\"\n",
    "#with open(outname, 'w') as csvfile:\n",
    "#    writer = csv.writer(csvfile,delimiter=\",\")\n",
    "#    writer.writerow([\"PopTraniningScore\", \"PopTestScore\", \"PopValidationScore\", \\\n",
    "#                    \"ConTraniningScore\", \"ConTestScore\", \"ConValidationScore\", \n",
    "#                    \"PopFraction\",\"ConFraction\"])\n",
    "#    writer.writerow([str(prec_pop_train), \\\n",
    "#                    str(prec_pop_test), \\\n",
    "#                    str(prec_pop_val), \\\n",
    "#                    str(prec_con_train), \\\n",
    "#                    str(prec_con_test), \\\n",
    "#                    str(prec_con_val), \\\n",
    "#                    str(Fpop),str(Fcon)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:32:57.741818Z",
     "start_time": "2019-09-30T17:32:57.730938Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[sklearn.utils.multiclass.unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "        'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "        'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "        'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "        'lines.markeredgewidth' : 3, 'lines.markersize': 6, 'figure.figsize': (8,8)}\n",
    "    mpl.rcParams.update(params)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T17:49:04.482745Z",
     "start_time": "2019-09-30T17:49:03.767179Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names = [\"Unpopular\", \"Popular\"]\n",
    "\n",
    "plot_confusion_matrix(popular_train, pred_pop_train, classes=class_names, normalize=True,\n",
    "                      title='Training Set')\n",
    "\n",
    "plot_confusion_matrix(popular_test, pred_pop_test, classes=class_names, normalize=True,\n",
    "                      title='Test Set')\n",
    "\n",
    "plot_confusion_matrix(popular_validation, pred_pop_val, classes=class_names, normalize=True,\n",
    "                      title='Validation Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experimental: perform regression including topics and sentiment features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Setup the Popularity and Controversiality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:55:46.272562Z",
     "start_time": "2019-09-26T20:55:46.253802Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build labels and features\n",
    "features = np.concatenate((predicted_topic_nmf,sentiment),axis=1)\n",
    "\n",
    "# Popularity of a post\n",
    "p = [0] * len(df[\"Popularity\"].values)\n",
    "popular = np.array(p); del p\n",
    "cutoff_pop = np.quantile(df[\"Popularity\"].values,q=0.5)\n",
    "selp0 = df[\"Popularity\"].values > cutoff_pop\n",
    "popular[selp0] = 1\n",
    "\n",
    "# Controversiality of a post\n",
    "c = [0] * len(df[\"upvote_ratio\"].values)\n",
    "controversial = np.array(c); del c\n",
    "inf = 0.25\n",
    "sup = 0.75\n",
    "selc0 = np.logical_and(inf <= df[\"upvote_ratio\"].values.astype(float), \\\n",
    "                       df[\"upvote_ratio\"].values.astype(float) <= sup)\n",
    "controversial[selc0] = 1\n",
    "selc1 = np.logical_or(df[\"upvote_ratio\"].values.astype(float) < inf, \\\n",
    "                      df[\"upvote_ratio\"].values.astype(float) > sup)\n",
    "\n",
    "Fpop = float(len(popular[selp0])/len(popular))\n",
    "Fcon = float(len(controversial[selc0])/len(controversial))\n",
    "\n",
    "print(\"Fraction of Popular Posts = \",float(len(popular[selp0])/len(popular)))\n",
    "print(\"Fraction of Controversial Posts = \",float(len(controversial[selc0])/len(controversial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T22:18:40.058436Z",
     "start_time": "2019-09-17T22:18:40.055549Z"
    },
    "hidden": true
   },
   "source": [
    "## Split in training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:55:48.602385Z",
     "start_time": "2019-09-26T20:55:48.555657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "train_size = 1.0 - test_size - validation_size\n",
    "\n",
    "X, featurep_test, Y, popular_test = \\\n",
    "    skmodsel.train_test_split(features,popular,test_size=test_size,stratify=popular)\n",
    "featurep_train, featurep_validation, popular_train, popular_validation = \\\n",
    "    skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "\n",
    "X, featurec_test, Y, controversial_test = \\\n",
    "    skmodsel.train_test_split(features,controversial,test_size=test_size,stratify=controversial)\n",
    "featurec_train, featurec_validation, controversial_train, controversial_validation = \\\n",
    "    skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:55:54.551721Z",
     "start_time": "2019-09-26T20:55:49.348205Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wp = [1.0] * featurep_train.shape[0]\n",
    "weight_pop = np.array(wp); del wp\n",
    "selp0 = weight_pop == 0\n",
    "weight_pop[selp0] = Fpop\n",
    "#weight_pop = {0: Fpop, 1: 1.0}\n",
    "\n",
    "wc = [1.0] * featurec_train.shape[0]\n",
    "weight_con = np.array(wc); del wc\n",
    "selc1 = weight_con == 0\n",
    "weight_con[selc1] = Fcon\n",
    "#weight_pop = {0: Fcon, 1: 1.0}\n",
    "\n",
    "popular_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                        n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "controversial_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                        n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "\n",
    "popular_gboost = popular_gboost.fit(featurep_train,popular_train, \\\n",
    "                       sample_weight=weight_pop)\n",
    "controversial_gboost = controversial_gboost.fit(featurec_train,controversial_train, \\\n",
    "                       sample_weight=weight_con)\n",
    "\n",
    "pred_pop_train = np.rint(popular_gboost.predict(featurep_train))\n",
    "pred_pop_test = np.rint(popular_gboost.predict(featurep_test))\n",
    "pred_pop_val = np.rint(popular_gboost.predict(featurep_validation))\n",
    "pred_con_train = np.rint(popular_gboost.predict(featurec_train))\n",
    "pred_con_test = np.rint(popular_gboost.predict(featurec_test))\n",
    "pred_con_val = np.rint(popular_gboost.predict(featurec_validation))\n",
    "\n",
    "prec_pop_train = sklearn.metrics.precision_score(popular_train,pred_pop_train)\n",
    "prec_pop_test = sklearn.metrics.precision_score(popular_test,pred_pop_test)\n",
    "prec_pop_val = sklearn.metrics.precision_score(popular_validation,pred_pop_val)\n",
    "prec_con_train = sklearn.metrics.precision_score(controversial_train,pred_con_train)\n",
    "prec_con_test = sklearn.metrics.precision_score(controversial_test,pred_con_test)\n",
    "prec_con_val = sklearn.metrics.precision_score(controversial_validation,pred_con_val)\n",
    "\n",
    "print(\"Gradient Boost for Popularity. Score on Training Set = \", prec_pop_train)\n",
    "print(\"Gradient Boost for Popularity. Score on Test Set = \", prec_pop_test)\n",
    "print(\"Gradient Boost for Popularity. Score on Validation Set = \", prec_pop_val,\"\\n\")\n",
    "\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_train)\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_test)\n",
    "print(\"Gradient Boost for Controversiality. Score on Training Set = \", prec_con_val,\"\\n\")\n",
    "\n",
    "outpath = \"./regression_models/\"\n",
    "outname = outpath+\"gboost_scores_nmf_wsent_prod_r-\"+subrname+\".csv\"\n",
    "with open(outname, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile,delimiter=\",\")\n",
    "    writer.writerow([\"PopTraniningScore\", \"PopTestScore\", \"PopValidationScore\", \\\n",
    "                    \"ConTraniningScore\", \"ConTestScore\", \"ConValidationScore\", \n",
    "                    \"PopFraction\",\"ConFraction\"])\n",
    "    writer.writerow([str(prec_pop_train), \\\n",
    "                    str(prec_pop_test), \\\n",
    "                    str(prec_pop_val), \\\n",
    "                    str(prec_con_train), \\\n",
    "                    str(prec_con_test), \\\n",
    "                    str(prec_con_val), \\\n",
    "                    str(Fpop),str(Fcon)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production model creation and optimization \n",
    "\n",
    "The full pipeline is run multiple times assuming different numbers of topics, then the best model in terms of compromise between precision and robustness is selected as the production model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:20.119857Z",
     "start_time": "2019-09-26T21:36:20.106773Z"
    }
   },
   "outputs": [],
   "source": [
    "if path.exists(\"nmf_models\"):\n",
    "    print(\"Directory nmf_models exists!\")\n",
    "else:\n",
    "    !mkdir nmf_models\n",
    "    print(\"Directory nmf_models created!\")\n",
    "    \n",
    "if path.exists(\"regression_models\"):\n",
    "    print(\"Directory regression_models exists!\")\n",
    "else:\n",
    "    !mkdir regression_models\n",
    "    print(\"Directory regression_models created!\")\n",
    "    \n",
    "if path.exists(\"pre_rendered_plots\"):\n",
    "    print(\"Directory pre_rendered_plots exists!\")\n",
    "else:\n",
    "    !mkdir regression_models\n",
    "    print(\"Directory pre_rendered_plots created!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:20.805463Z",
     "start_time": "2019-09-26T21:36:20.802082Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''Function to lemmatize text'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:21.156008Z",
     "start_time": "2019-09-26T21:36:21.152473Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''Function to pre-process text'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:21.634107Z",
     "start_time": "2019-09-26T21:36:21.616916Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_models(processed_subm_sentences,num_topics):\n",
    "    '''\n",
    "    This function runs the whole pipeline to train \n",
    "    the NMF topic model and the regression models for\n",
    "    popularity and controversiality.\n",
    "    '''\n",
    "    # NMF model\n",
    "    vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "    x_counts = vectorizer.fit_transform(processed_subm_sentences)\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    x_tfidf = transformer.fit_transform(x_counts)\n",
    "    xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "    nmf_model = NMF(n_components=num_topics,init='nndsvd',alpha=0.1)\n",
    "    y = nmf_model.fit_transform(xtfidf_norm)\n",
    "    features = normalize(y, norm='l1', axis=1)\n",
    "    \n",
    "    # Popularity of a post\n",
    "    p = [0] * len(df[\"Popularity\"].values)\n",
    "    popular = np.array(p); del p\n",
    "    cutoff_pop = np.quantile(df[\"Popularity\"].values,q=0.5)\n",
    "    selp0 = df[\"Popularity\"].values > cutoff_pop\n",
    "    popular[selp0] = 1\n",
    "\n",
    "    # Controversiality of a post\n",
    "    c = [0] * len(df[\"upvote_ratio\"].values)\n",
    "    controversial = np.array(c); del c\n",
    "    inf = 0.25\n",
    "    sup = 0.75\n",
    "    selc0 = np.logical_and(inf <= df[\"upvote_ratio\"].values.astype(float), \\\n",
    "                       df[\"upvote_ratio\"].values.astype(float) <= sup)\n",
    "    controversial[selc0] = 1\n",
    "    \n",
    "    # Fraction of popular and controversial\n",
    "    Fpop = float(len(popular[selp0])/len(popular))\n",
    "    Fcon = float(len(controversial[selc0])/len(controversial))\n",
    "    \n",
    "    # Training/Test/Validation\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.2\n",
    "    train_size = 1.0 - test_size - validation_size\n",
    "    X, featurep_test, Y, popular_test = \\\n",
    "        skmodsel.train_test_split(features,popular,test_size=test_size,stratify=popular)\n",
    "    featurep_train, featurep_validation, popular_train, popular_validation = \\\n",
    "        skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "    X, featurec_test, Y, controversial_test = \\\n",
    "        skmodsel.train_test_split(features,controversial,test_size=test_size,stratify=controversial)\n",
    "    featurec_train, featurec_validation, controversial_train, controversial_validation = \\\n",
    "        skmodsel.train_test_split(X,Y,test_size=validation_size/(1.0-test_size),stratify=Y)\n",
    "    del X,Y\n",
    "    \n",
    "    # Sample Weights\n",
    "    wp = [1.0] * featurep_train.shape[0]\n",
    "    weight_pop = np.array(wp); del wp\n",
    "    selp0 = weight_pop == 0\n",
    "    weight_pop[selp0] == Fpop\n",
    "\n",
    "    wc = [1.0] * featurec_train.shape[0]\n",
    "    weight_con = np.array(wc); del wc\n",
    "    selc1 = weight_con == 0\n",
    "    weight_con[selc1] = Fcon\n",
    "\n",
    "    #\n",
    "    popular_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                     n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "    controversial_gboost = skensemble.GradientBoostingRegressor( \\\n",
    "                     n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "\n",
    "    popular_gboost = popular_gboost.fit(featurep_train,popular_train, \\\n",
    "                     sample_weight=weight_pop)\n",
    "    controversial_gboost = controversial_gboost.fit(featurec_train,controversial_train, \\\n",
    "                     sample_weight=weight_con)\n",
    "\n",
    "    pred_pop_train = np.rint(popular_gboost.predict(featurep_train))\n",
    "    pred_pop_test = np.rint(popular_gboost.predict(featurep_test))\n",
    "    pred_pop_val = np.rint(popular_gboost.predict(featurep_validation))\n",
    "    pred_con_train = np.rint(popular_gboost.predict(featurec_train))\n",
    "    pred_con_test = np.rint(popular_gboost.predict(featurec_test))\n",
    "    pred_con_val = np.rint(popular_gboost.predict(featurec_validation))\n",
    "\n",
    "    prec_pop_train = sklearn.metrics.precision_score(popular_train,pred_pop_train)\n",
    "    prec_pop_test = sklearn.metrics.precision_score(popular_test,pred_pop_test)\n",
    "    prec_pop_val = sklearn.metrics.precision_score(popular_validation,pred_pop_val)\n",
    "    prec_con_train = sklearn.metrics.precision_score(controversial_train,pred_con_train)\n",
    "    prec_con_test = sklearn.metrics.precision_score(controversial_test,pred_con_test)\n",
    "    prec_con_val = sklearn.metrics.precision_score(controversial_validation,pred_con_val)\n",
    "    \n",
    "    return vectorizer,transformer,nmf_model, \\\n",
    "           popular_gboost, \\\n",
    "           prec_pop_train,prec_pop_test,prec_pop_val, \\\n",
    "           controversial_gboost, \\\n",
    "           prec_con_train,prec_con_test,prec_con_val,Fpop,Fcon     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:22.202863Z",
     "start_time": "2019-09-26T21:36:22.195002Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_models(subr,sel_num_topics,vectorizer,transformer,nmf_model, \\\n",
    "                gb_popular,gb_controversial,precision_pop,precision_con, \\\n",
    "                Fcon,Fpop):\n",
    "\n",
    "    # Save Topic Model\n",
    "    outpath = \"./nmf_models\"\n",
    "    outfile = outpath+\"/count_vectorizer_prod_r-\"+subr\n",
    "    pickle.dump(vectorizer, open(outfile, 'wb'))\n",
    "    outfile = outpath+\"/tfidf_transformer_prod_r-\"+subr\n",
    "    pickle.dump(transformer, open(outfile, 'wb'))    \n",
    "    outfile = outpath+\"/nmf_model_prod_r-\"+subr\n",
    "    pickle.dump(nmf_model, open(outfile, 'wb'))\n",
    "\n",
    "    # Save Regression Model\n",
    "    outpath = \"./regression_models\"\n",
    "    outfile = outpath+\"/gbreg_popular_prod_r-\"+subr\n",
    "    pickle.dump(gb_popular, open(outfile, 'wb'))\n",
    "    outfile = outpath+\"/gbreg_controversial_prod_r-\"+subr\n",
    "    pickle.dump(gb_controversial, open(outfile, 'wb'))\n",
    "    \n",
    "    # Save Precisions and Fractions\n",
    "    outpath = \"./regression_models\"\n",
    "    outfile = outpath+\"/gbprecision_nmf_prod_r-\"+subr+\".csv\"\n",
    "    with open(outfile, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile,delimiter=\",\")\n",
    "        writer.writerow([\"PopTraniningScore\", \"PopTestScore\", \"PopValidationScore\", \\\n",
    "                    \"ConTraniningScore\", \"ConTestScore\", \"ConValidationScore\", \n",
    "                    \"PopFraction\",\"ConFraction\"])\n",
    "        writer.writerow([str(precision_pop[0]), \\\n",
    "                         str(precision_pop[1]), \\\n",
    "                         str(precision_pop[2]), \\\n",
    "                         str(precision_con[0]), \\\n",
    "                         str(precision_con[1]), \\\n",
    "                         str(precision_con[2]), \\\n",
    "                         str(Fpop),str(Fcon)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:35.436874Z",
     "start_time": "2019-09-26T21:36:35.427009Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_precision(subr,sel_num_topics,num_topics_list,prec_matrix):\n",
    "    params={'font.size': 20,'axes.labelsize': 20,'legend.fontsize': 18,\n",
    "            'xtick.labelsize': 20,'ytick.labelsize': 20,'lines.linewidth': 4,'axes.linewidth': 3,\n",
    "            'xtick.major.width': 3,'ytick.major.width': 3,'xtick.minor.width': 3,'ytick.minor.width': 3,\n",
    "            'xtick.major.size': 7,'ytick.major.size': 7,'xtick.minor.size': 5,'ytick.minor.size': 5,\n",
    "            'lines.markeredgewidth' : 3, 'lines.markersize': 6}\n",
    "    mpl.rcParams.update(params)\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(121)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.axis([0,101,0.0,1.0])\n",
    "    plt.title(\"GBoosting on Popolarity\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,0,0],label=\"r/\"+subr+\" Training Set\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,0,1],label=\"r/\"+subr+\" Test Set\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,0,2],label=\"r/\"+subr+\" Validation Set\")\n",
    "    plt.plot([sel_num_topics,sel_num_topics],[0.35,2],'r-',label=\"Selected Model\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.subplot(122)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.axis([0,101,0.0,1.0])\n",
    "    plt.title(\"GBoosting on Controversiality\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,1,0],label=\"r/\"+subr+\" Training Set\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,1,1],label=\"r/\"+subr+\" Test Set\")\n",
    "    plt.plot(num_topics_list,prec_matrix[:,1,2],label=\"r/\"+subr+\" Validation Set\")\n",
    "    #plt.plot([sel_num_topics,sel_num_topics],[0.35,2],'r-',label=\"Selected Model\")\n",
    "    #plt.legend(loc=3)\n",
    "    plt.savefig(\"./pre_rendered_plots/precision_r-\"+subr+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:35.963227Z",
     "start_time": "2019-09-26T21:36:35.954469Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_popularity(subr,df):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.xscale(\"linear\")\n",
    "    plt.yscale(\"log\")\n",
    "    xmin = -0.3\n",
    "    xmedian = np.quantile(np.log10(df[\"Popularity\"].values),0.5)\n",
    "    xmax = np.log10(np.max(df[\"Popularity\"].values))\n",
    "    ymin = 1.01\n",
    "    isel = df[\"Popularity\"].values == 1\n",
    "    ymax = 5*len(df[\"Popularity\"].values[isel])\n",
    "    plt.axis([xmin,1.05*xmax,ymin,ymax])\n",
    "    plt.xlabel(\"Popularity\")\n",
    "    plt.ylabel(\"Number of Submissions\")\n",
    "    plt.xticks(np.arange(0,int(1.05*xmax)+1),labels=[\"%d\"%(10**i) for i in np.arange(0,int(1.05*xmax)+1)])\n",
    "    plt.axvspan(-1, xmedian, facecolor='gray',alpha=0.5,label=\"Unpopular Range\")\n",
    "    plt.hist(np.log10(df[\"Popularity\"].values),bins=int((xmax-xmin)*5),range=[xmin,xmax],rwidth=0.8,color='lightblue',edgecolor='black',alpha=1,label=\"r/\"+subr)\n",
    "    plt.plot([xmedian,xmedian],[ymin,ymax],'r-',label=\"Popularity Threshold\")\n",
    "    plt.title(\"Popularity = #Comments + #Upvotes\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.savefig(\"./pre_rendered_plots/popularity_hist_r-\"+subr+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T21:36:36.192526Z",
     "start_time": "2019-09-26T21:36:36.184948Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_controversiality(subr,df):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    plt.xscale(\"linear\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Controversiality\")\n",
    "    plt.ylabel(\"Number of Submissions\")\n",
    "    ymin = 1.01\n",
    "    isel = df[\"Controversiality\"].values < 0.2\n",
    "    ymax = 5.*len(df[\"Controversiality\"].values[isel])\n",
    "    plt.axis([-0.01,1.0,ymin,ymax])\n",
    "    plt.xticks(np.arange(0,1.2,0.2),labels=[\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1.0\"])\n",
    "    plt.axvspan(0.5, 1.0, facecolor='gray',alpha=0.5,label=\"Controversial Range\")\n",
    "    plt.plot([0.5,0.5],[0,1e8],\"r-\",linewidth=5,label=\"Controversiality Threshold\")\n",
    "    histdata = [df[\"Controversiality\"].values]\n",
    "    plt.hist(histdata,bins=20,range=[0,1],rwidth=0.8,color='lightblue',edgecolor='black',alpha=1,label=\"r/\"+subr)\n",
    "    plt.title(\"Controversiality = 1-|2*#Upvotes/(#Upvotes+#Downvotes)-1|\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.savefig(\"./pre_rendered_plots/controversiality_hist_r-\"+subr+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T15:23:40.143185Z",
     "start_time": "2019-09-27T15:23:40.126905Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_model(subr,df,incr_thr,diff_thr):\n",
    "    '''\n",
    "    This function finds the optimal model that \n",
    "    does not overfit the data and for which \n",
    "    precision maximized. \n",
    "    '''    \n",
    "    # Preprocess titles \n",
    "    processed_subm = df[\"title+selftext\"].map(preprocess)\n",
    "    processed_subm_sentences = [' '.join(text) for text in processed_subm]\n",
    "\n",
    "    # Storage structures for the models\n",
    "    num_topics_list = [2,5,10,20,30,40,50,60,70,80,90,100]\n",
    "    prec_matrix = np.zeros((len(num_topics_list),2,3))\n",
    "    old_prec_pop_train = 0.01\n",
    "    old_prec_con_train = 0.01\n",
    "    increment_pop_train = 0.0\n",
    "    increment_con_train = 0.0\n",
    "    diff_pop = 0.0\n",
    "    diff_con = 0.0\n",
    "    sel_num_topics = 0\n",
    "    selected = False\n",
    "    for i,num_topics in enumerate(num_topics_list):\n",
    "        vec,trans,nmf, \\\n",
    "        gbpop,prec_pop_train, \\\n",
    "        prec_pop_test,prec_pop_val, \\\n",
    "        gbcon,prec_con_train, \\\n",
    "        prec_con_test,prec_con_val, \\\n",
    "        Fpop,Fcon = create_models(processed_subm_sentences,num_topics)\n",
    "    \n",
    "        # Precision Matrix \n",
    "        prec_matrix[i,0,0] = prec_pop_train\n",
    "        prec_matrix[i,0,1] = prec_pop_test\n",
    "        prec_matrix[i,0,2] = prec_pop_val\n",
    "        prec_matrix[i,1,0] = prec_con_train\n",
    "        prec_matrix[i,1,1] = prec_con_test\n",
    "        prec_matrix[i,1,2] = prec_con_val  \n",
    "\n",
    "        # Precision relative increment\n",
    "        increment_pop_train = (prec_pop_train-old_prec_pop_train)/old_prec_pop_train\n",
    "        increment_con_train = (prec_con_train-old_prec_con_train)/old_prec_con_train\n",
    "    \n",
    "        # Relative ifference between training and test/validation test\n",
    "        diff_pop = (prec_pop_train-0.5*(prec_pop_test+prec_pop_val))/prec_pop_train\n",
    "        diff_con = (prec_con_train-0.5*(prec_con_test+prec_con_val))/prec_con_train   \n",
    "    \n",
    "        # Update old precisions\n",
    "        old_prec_pop_train = prec_pop_train\n",
    "        old_prec_con_train = prec_con_train\n",
    "\n",
    "        # Selec model\n",
    "        if i == 0 or increment_pop_train > incr_thr and increment_con_train > incr_thr and \\\n",
    "            diff_pop < diff_thr and diff_con < diff_thr:\n",
    "        \n",
    "            sel_num_topics = num_topics\n",
    "            vectorizer = vec\n",
    "            transformer = trans\n",
    "            nmf_model = nmf\n",
    "            gb_popular = gbpop\n",
    "            gb_controversial = gbcon\n",
    "            precision_pop = [prec_pop_train,prec_pop_test,prec_pop_test]\n",
    "            precision_con = [prec_con_train,prec_con_test,prec_con_val]\n",
    "            selected = True\n",
    "    \n",
    "    try:\n",
    "        save_models(subr,sel_num_topics,vectorizer,transformer,nmf_model, \\\n",
    "                    gb_popular,gb_controversial,precision_pop,precision_con, \\\n",
    "                    Fpop,Fcon)\n",
    "        print(\"Model created and saved\")\n",
    "    except:\n",
    "        print(\"Unable to select a good model...\")\n",
    "    \n",
    "    plot_precision(subr,sel_num_topics,num_topics_list,prec_matrix)\n",
    "    plot_popularity(subr,df)\n",
    "    plot_controversiality(subr,df)\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T15:23:40.680780Z",
     "start_time": "2019-09-27T15:23:40.674432Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_subr(subr):\n",
    "    inpath = \"./subreddit_data/\"\n",
    "    fname = inpath+\"r-\"+subr+\"-export.pkl\"\n",
    "    dfraw = pd.read_pickle(fname)\n",
    "\n",
    "    # Filter out removed posts\n",
    "    sel = dfraw[\"selftext\"].str.strip() == \"[removed]\"\n",
    "    dfraw[\"selftext\"][sel] = \"\"\n",
    "    dfraw[\"title+selftext\"] = dfraw[\"title\"]+\" \"+dfraw[\"selftext\"]\n",
    "    dfraw[\"Popularity\"] = dfraw[\"score\"]+dfraw[\"numComms\"]\n",
    "    dfraw[\"Controversiality\"] = 1.0-np.abs((dfraw[\"upvote_ratio\"].values.astype(float)-0.5)/0.5)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Filter out posts with less than 3 words\n",
    "    n_w = [len(dfraw[\"title+selftext\"][i].split()) for i in range(0,len(dfraw[\"title+selftext\"]))]\n",
    "    dfraw[\"n_words\"] = n_w\n",
    "    sel = dfraw[\"n_words\"] > 3\n",
    "    df = dfraw[sel]    \n",
    "    \n",
    "    print(\"Data Loaded\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T17:57:33.552346Z",
     "start_time": "2019-09-27T16:39:13.271261Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop on all selected subreddits and create a model for them\n",
    "subr_list = [\n",
    "             \"changemyview\", \\\n",
    "             \"politicaldiscussion\", \\\n",
    "             \"history\", \\\n",
    "             \"quotes\", \\\n",
    "             \"compsci\", \\\n",
    "             \"parenting\" , \\\n",
    "             \"confession\", \\\n",
    "             \"advice\"\n",
    "            ]\n",
    "\n",
    "incr_thr = 1.25e-2\n",
    "diff_thr = 1.0e3\n",
    "\n",
    "for subr in subr_list:\n",
    "    df = load_subr(subr)\n",
    "    success = optimal_model(subr,df,incr_thr,diff_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
