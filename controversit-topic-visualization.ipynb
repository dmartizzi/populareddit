{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: create a visualization of a topic model with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T18:51:51.773514Z",
     "start_time": "2019-09-26T18:51:47.684426Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.test.utils import datapath\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "import sklearn\n",
    "import sklearn.model_selection as skmodsel\n",
    "import sklearn.linear_model as sklinmod\n",
    "import sklearn.ensemble as skensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T18:51:51.782752Z",
     "start_time": "2019-09-26T18:51:51.775596Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''Function to lemmatize text'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T18:51:51.789715Z",
     "start_time": "2019-09-26T18:51:51.784788Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''Function to pre-process text'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T18:51:51.804007Z",
     "start_time": "2019-09-26T18:51:51.791695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experimental: big ugly function\n",
    "def create_topic_viz(subrname):\n",
    "\n",
    "    # Subreddit data\n",
    "    inpath = \"./subreddit_data/\"\n",
    "    fname = inpath+\"r-\"+subrname+\"-export.pkl\"\n",
    "    dfraw = pd.read_pickle(fname)\n",
    "\n",
    "    # Filter out removed posts\n",
    "    sel = dfraw[\"selftext\"].str.strip() == \"[removed]\"\n",
    "    dfraw[\"selftext\"][sel] = \"\"\n",
    "    dfraw[\"title+selftext\"] = dfraw[\"title\"]+\" \"+dfraw[\"selftext\"]\n",
    "    dfraw[\"Popularity\"] = dfraw[\"score\"]+dfraw[\"numComms\"]\n",
    "\n",
    "    # Filter out posts with less than 3 words\n",
    "    n_w = [len(dfraw[\"title+selftext\"][i].split()) for i in range(0,len(dfraw[\"title+selftext\"]))]\n",
    "    dfraw[\"n_words\"] = n_w\n",
    "    sel = dfraw[\"n_words\"] > 3\n",
    "    df = dfraw[sel]    \n",
    "\n",
    "    outpath = \"./nmf_models/\"\n",
    "    outfile = outpath+\"nmf_model_prod_r-\"+subrname\n",
    "    nmf_model = pickle.load(open(outfile, 'rb'))\n",
    "\n",
    "    num_topics = nmf_model.components_.shape[0]\n",
    "    num_words = nmf_model.components_.shape[1]\n",
    "\n",
    "    print(\"r/\"+subrname+\" Data and Models loaded\")\n",
    "\n",
    "    # Process sentences\n",
    "    processed_subm = df[\"title+selftext\"].map(preprocess)\n",
    "    processed_subm_sentences = [' '.join(text) for text in processed_subm]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_features=num_words, smooth_idf=True)\n",
    "    x_tfidf = tfidf_vectorizer.fit_transform(processed_subm_sentences)\n",
    "    x_tfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "    x_tfidf_norm = x_tfidf_norm.toarray()\n",
    "    cc = np.sum(x_tfidf_norm,axis=1)\n",
    "    isel = x_tfidf_norm > -1.0\n",
    "    for i in range(0,len(x_tfidf_norm[:,0])):\n",
    "        cc = np.sum(x_tfidf_norm[i,:])\n",
    "        if cc == 0.0:\n",
    "            isel[i,:] = False\n",
    "    size = int(x_tfidf_norm[isel].shape[0]/num_words)\n",
    "    x_tfidf_norm1 = np.matrix(x_tfidf_norm[isel].reshape((size,num_words)))\n",
    "    nmf_model = NMF(n_components=num_topics,init='nndsvd',alpha=0.1)\n",
    "    y = nmf_model.fit_transform(x_tfidf_norm1)\n",
    "    features = normalize(y, norm='l1', axis=1)\n",
    "\n",
    "    # Create visualization and save it\n",
    "    pyLDAvis.enable_notebook()\n",
    "    viz = pyLDAvis.sklearn.prepare(nmf_model, x_tfidf_norm1, tfidf_vectorizer)\n",
    "    pyLDAvis.save_html(viz,\"./controversit_app/templates/topic_analysis_r-\"+subrname+\".html\")\n",
    "\n",
    "    print(\"r/\"+subrname+\" Topic Data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T18:51:57.997792Z",
     "start_time": "2019-09-26T18:51:51.808526Z"
    }
   },
   "outputs": [],
   "source": [
    "subr_list = [\n",
    "             \"compsci\", \\\n",
    "             \"politicaldiscussion\", \\\n",
    "             \"history\", \\\n",
    "             \"quotes\", \\\n",
    "             \"changemyview\", \\\n",
    "             \"parenting\" , \\\n",
    "             \"confession\", \\\n",
    "             \"advice\"\n",
    "            ]\n",
    "\n",
    "for subr in subr_list:\n",
    "    create_topic_viz(subr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
