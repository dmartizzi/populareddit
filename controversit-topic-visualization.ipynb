{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: create a visualization of a topic model with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:59:20.767402Z",
     "start_time": "2019-09-25T23:59:00.326644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dmartizzi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.test.utils import datapath\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "import sklearn\n",
    "import sklearn.model_selection as skmodsel\n",
    "import sklearn.linear_model as sklinmod\n",
    "import sklearn.ensemble as skensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:59:20.776657Z",
     "start_time": "2019-09-25T23:59:20.769479Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''Function to lemmatize text'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:59:20.782094Z",
     "start_time": "2019-09-25T23:59:20.778692Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''Function to pre-process text'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T00:00:13.506642Z",
     "start_time": "2019-09-26T00:00:13.494628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experimental: big ugly function\n",
    "def create_topic_viz(subrname):\n",
    "\n",
    "    # Subreddit data\n",
    "    inpath = \"./subreddit_data/\"\n",
    "    fname = inpath+\"r-\"+subrname+\"-export.pkl\"\n",
    "    dfraw = pd.read_pickle(fname)\n",
    "\n",
    "    # Filter out removed posts\n",
    "    sel = dfraw[\"selftext\"].str.strip() == \"[removed]\"\n",
    "    dfraw[\"selftext\"][sel] = \"\"\n",
    "    dfraw[\"title+selftext\"] = dfraw[\"title\"]+\" \"+dfraw[\"selftext\"]\n",
    "    dfraw[\"Popularity\"] = dfraw[\"score\"]+dfraw[\"numComms\"]\n",
    "\n",
    "    # Filter out posts with less than 3 words\n",
    "    dfraw[\"n_words\"] = dfraw[\"title+selftext\"][:].str.split()\n",
    "    for i in range(0,len(dfraw[\"n_words\"])):\n",
    "        dfraw[\"n_words\"][i] = len(dfraw[\"n_words\"][i])\n",
    "\n",
    "    sel = dfraw[\"n_words\"] > 3\n",
    "    df = dfraw[sel]    \n",
    "\n",
    "    outpath = \"./nmf_models/\"\n",
    "    outfile = outpath+\"nmf_model_prod_r-\"+subrname\n",
    "    nmf_model = pickle.load(open(outfile, 'rb'))\n",
    "\n",
    "    num_topics = nmf_model.components_.shape[0]\n",
    "    num_words = nmf_model.components_.shape[1]\n",
    "\n",
    "    print(\"r/\"+subrname+\" Data and Models loaded\")\n",
    "\n",
    "    # Process sentences\n",
    "    processed_subm = df[\"title+selftext\"].map(preprocess)\n",
    "    processed_subm_sentences = [' '.join(text) for text in processed_subm]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_features=num_words, smooth_idf=True)\n",
    "    x_tfidf = tfidf_vectorizer.fit_transform(processed_subm_sentences)\n",
    "    x_tfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "    x_tfidf_norm = x_tfidf_norm.toarray()\n",
    "    cc = np.sum(x_tfidf_norm,axis=1)\n",
    "    isel = x_tfidf_norm > -1.0\n",
    "    for i in range(0,len(x_tfidf_norm[:,0])):\n",
    "        cc = np.sum(x_tfidf_norm[i,:])\n",
    "        if cc == 0.0:\n",
    "            isel[i,:] = False\n",
    "    size = int(x_tfidf_norm[isel].shape[0]/num_words)\n",
    "    x_tfidf_norm1 = np.matrix(x_tfidf_norm[isel].reshape((size,num_words)))\n",
    "    nmf_model = NMF(n_components=num_topics,init='nndsvd',alpha=0.1)\n",
    "    y = nmf_model.fit_transform(x_tfidf_norm1)\n",
    "    features = normalize(y, norm='l1', axis=1)\n",
    "\n",
    "    # Create visualization and save it\n",
    "    pyLDAvis.enable_notebook()\n",
    "    viz = pyLDAvis.sklearn.prepare(nmf_model, x_tfidf_norm1, tfidf_vectorizer)\n",
    "    pyLDAvis.save_html(viz,\"./controversit_app/templates/topic_analysis_r-\"+subrname+\".html\")\n",
    "\n",
    "    print(\"r/\"+subrname+\" Topic Data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T01:31:09.727916Z",
     "start_time": "2019-09-26T00:00:14.140747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/compsci Data and Models loaded\n",
      "r/compsci Topic Data saved\n",
      "r/politicaldiscussion Data and Models loaded\n",
      "r/politicaldiscussion Topic Data saved\n",
      "r/history Data and Models loaded\n",
      "r/history Topic Data saved\n",
      "r/quotes Data and Models loaded\n",
      "r/quotes Topic Data saved\n",
      "r/changemyview Data and Models loaded\n",
      "r/changemyview Topic Data saved\n",
      "r/parenting Data and Models loaded\n",
      "r/parenting Topic Data saved\n",
      "r/confession Data and Models loaded\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in doc_topic_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-22650254851f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubr_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcreate_topic_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-549daea4790a>\u001b[0m in \u001b[0;36mcreate_topic_viz\u001b[0;34m(subrname)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Create visualization and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mviz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tfidf_norm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./controversit_app/templates/topic_analysis_r-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubrname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
     ]
    }
   ],
   "source": [
    "subr_list = [\n",
    "             \"compsci\", \\\n",
    "             \"politicaldiscussion\", \\\n",
    "             \"history\", \\\n",
    "             \"quotes\", \\\n",
    "             \"changemyview\", \\\n",
    "             \"parenting\" , \\\n",
    "             \"confession\", \\\n",
    "             \"advice\"\n",
    "            ]\n",
    "\n",
    "for subr in subr_list:\n",
    "    create_topic_viz(subr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
