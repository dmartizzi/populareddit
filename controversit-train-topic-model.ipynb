{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Natural Language Processing model for the topics in a given subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:06:50.664707Z",
     "start_time": "2019-09-18T17:06:50.437205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dmartizzi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import ciso8601\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.test.utils import datapath\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "import sklearn.model_selection as skmodsel\n",
    "import sklearn.linear_model as sklinmod\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a pickle file (temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:03.314708Z",
     "start_time": "2019-09-18T17:06:55.252745Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>sub_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>numComms</th>\n",
       "      <th>flair</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>title+selftext</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abh541</td>\n",
       "      <td>“May All Your Troubles Last As Long As Your Ne...</td>\n",
       "      <td>WildeAquarius</td>\n",
       "      <td>2019-01-01 02:15:10</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abh54...</td>\n",
       "      <td>/r/quotes/comments/abh541/may_all_your_trouble...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.88</td>\n",
       "      <td>“May All Your Troubles Last As Long As Your Ne...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abh956</td>\n",
       "      <td>\"The penalty for laughing in a courtroom is si...</td>\n",
       "      <td>ScrambledShow</td>\n",
       "      <td>2019-01-01 02:37:19</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abh95...</td>\n",
       "      <td>/r/quotes/comments/abh956/the_penalty_for_laug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.94</td>\n",
       "      <td>\"The penalty for laughing in a courtroom is si...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abh957</td>\n",
       "      <td>\"Man is a clever animal who behaves like an im...</td>\n",
       "      <td>ScrambledShow</td>\n",
       "      <td>2019-01-01 02:37:19</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abh95...</td>\n",
       "      <td>/r/quotes/comments/abh957/man_is_a_clever_anim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.96</td>\n",
       "      <td>\"Man is a clever animal who behaves like an im...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abh95a</td>\n",
       "      <td>\"Beauty is in the eye of the beholder and it m...</td>\n",
       "      <td>ScrambledShow</td>\n",
       "      <td>2019-01-01 02:37:19</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abh95...</td>\n",
       "      <td>/r/quotes/comments/abh95a/beauty_is_in_the_eye...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.98</td>\n",
       "      <td>\"Beauty is in the eye of the beholder and it m...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abh95c</td>\n",
       "      <td>\"To say that a man is made up of certain chemi...</td>\n",
       "      <td>ScrambledShow</td>\n",
       "      <td>2019-01-01 02:37:20</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abh95...</td>\n",
       "      <td>/r/quotes/comments/abh95c/to_say_that_a_man_is...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.93</td>\n",
       "      <td>\"To say that a man is made up of certain chemi...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abhl67</td>\n",
       "      <td>\"If we confine something to one place, it will...</td>\n",
       "      <td>i-Wayfarer</td>\n",
       "      <td>2019-01-01 03:39:51</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abhl6...</td>\n",
       "      <td>/r/quotes/comments/abhl67/if_we_confine_someth...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.93</td>\n",
       "      <td>\"If we confine something to one place, it will...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abhwat</td>\n",
       "      <td>\"We become what we think about.\" - Earl Nighti...</td>\n",
       "      <td>finneganishome</td>\n",
       "      <td>2019-01-01 04:37:14</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abhwa...</td>\n",
       "      <td>/r/quotes/comments/abhwat/we_become_what_we_th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.96</td>\n",
       "      <td>\"We become what we think about.\" - Earl Nighti...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abhyke</td>\n",
       "      <td>If you don't like something, change it. If you...</td>\n",
       "      <td>osamanasim</td>\n",
       "      <td>2019-01-01 04:48:54</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abhyk...</td>\n",
       "      <td>/r/quotes/comments/abhyke/if_you_dont_like_som...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.82</td>\n",
       "      <td>If you don't like something, change it. If you...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abhzmc</td>\n",
       "      <td>\"All straight men are into traps - some are ju...</td>\n",
       "      <td>DiSyndra</td>\n",
       "      <td>2019-01-01 04:54:28</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abhzm...</td>\n",
       "      <td>/r/quotes/comments/abhzmc/all_straight_men_are...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.57</td>\n",
       "      <td>\"All straight men are into traps - some are ju...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quotes</td>\n",
       "      <td>abi7yh</td>\n",
       "      <td>Hold fast to dreams, for if dreams die, life i...</td>\n",
       "      <td>Letitgo23607</td>\n",
       "      <td>2019-01-01 05:34:15</td>\n",
       "      <td>https://www.reddit.com/r/quotes/comments/abi7y...</td>\n",
       "      <td>/r/quotes/comments/abi7yh/hold_fast_to_dreams_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.9</td>\n",
       "      <td>Hold fast to dreams, for if dreams die, life i...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  sub_id                                              title  \\\n",
       "0    quotes  abh541  “May All Your Troubles Last As Long As Your Ne...   \n",
       "1    quotes  abh956  \"The penalty for laughing in a courtroom is si...   \n",
       "2    quotes  abh957  \"Man is a clever animal who behaves like an im...   \n",
       "3    quotes  abh95a  \"Beauty is in the eye of the beholder and it m...   \n",
       "4    quotes  abh95c  \"To say that a man is made up of certain chemi...   \n",
       "5    quotes  abhl67  \"If we confine something to one place, it will...   \n",
       "6    quotes  abhwat  \"We become what we think about.\" - Earl Nighti...   \n",
       "7    quotes  abhyke  If you don't like something, change it. If you...   \n",
       "8    quotes  abhzmc  \"All straight men are into traps - some are ju...   \n",
       "9    quotes  abi7yh  Hold fast to dreams, for if dreams die, life i...   \n",
       "\n",
       "           author              created  \\\n",
       "0   WildeAquarius  2019-01-01 02:15:10   \n",
       "1   ScrambledShow  2019-01-01 02:37:19   \n",
       "2   ScrambledShow  2019-01-01 02:37:19   \n",
       "3   ScrambledShow  2019-01-01 02:37:19   \n",
       "4   ScrambledShow  2019-01-01 02:37:20   \n",
       "5      i-Wayfarer  2019-01-01 03:39:51   \n",
       "6  finneganishome  2019-01-01 04:37:14   \n",
       "7      osamanasim  2019-01-01 04:48:54   \n",
       "8        DiSyndra  2019-01-01 04:54:28   \n",
       "9    Letitgo23607  2019-01-01 05:34:15   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/quotes/comments/abh54...   \n",
       "1  https://www.reddit.com/r/quotes/comments/abh95...   \n",
       "2  https://www.reddit.com/r/quotes/comments/abh95...   \n",
       "3  https://www.reddit.com/r/quotes/comments/abh95...   \n",
       "4  https://www.reddit.com/r/quotes/comments/abh95...   \n",
       "5  https://www.reddit.com/r/quotes/comments/abhl6...   \n",
       "6  https://www.reddit.com/r/quotes/comments/abhwa...   \n",
       "7  https://www.reddit.com/r/quotes/comments/abhyk...   \n",
       "8  https://www.reddit.com/r/quotes/comments/abhzm...   \n",
       "9  https://www.reddit.com/r/quotes/comments/abi7y...   \n",
       "\n",
       "                                           permalink  score  numComms flair  \\\n",
       "0  /r/quotes/comments/abh541/may_all_your_trouble...      1         0   NaN   \n",
       "1  /r/quotes/comments/abh956/the_penalty_for_laug...      1         0   NaN   \n",
       "2  /r/quotes/comments/abh957/man_is_a_clever_anim...      1         0   NaN   \n",
       "3  /r/quotes/comments/abh95a/beauty_is_in_the_eye...      1         0   NaN   \n",
       "4  /r/quotes/comments/abh95c/to_say_that_a_man_is...      1         0   NaN   \n",
       "5  /r/quotes/comments/abhl67/if_we_confine_someth...      1         0   NaN   \n",
       "6  /r/quotes/comments/abhwat/we_become_what_we_th...      1         1   NaN   \n",
       "7  /r/quotes/comments/abhyke/if_you_dont_like_som...      1         0   NaN   \n",
       "8  /r/quotes/comments/abhzmc/all_straight_men_are...      1         0   NaN   \n",
       "9  /r/quotes/comments/abi7yh/hold_fast_to_dreams_...      1         0   NaN   \n",
       "\n",
       "  selftext upvote_ratio                                     title+selftext  \\\n",
       "0                  0.88  “May All Your Troubles Last As Long As Your Ne...   \n",
       "1                  0.94  \"The penalty for laughing in a courtroom is si...   \n",
       "2                  0.96  \"Man is a clever animal who behaves like an im...   \n",
       "3                  0.98  \"Beauty is in the eye of the beholder and it m...   \n",
       "4                  0.93  \"To say that a man is made up of certain chemi...   \n",
       "5                  0.93  \"If we confine something to one place, it will...   \n",
       "6                  0.96  \"We become what we think about.\" - Earl Nighti...   \n",
       "7                  0.82  If you don't like something, change it. If you...   \n",
       "8                  0.57  \"All straight men are into traps - some are ju...   \n",
       "9                   0.9  Hold fast to dreams, for if dreams die, life i...   \n",
       "\n",
       "  n_words  \n",
       "0      15  \n",
       "1      30  \n",
       "2      13  \n",
       "3      30  \n",
       "4      31  \n",
       "5      39  \n",
       "6       9  \n",
       "7      19  \n",
       "8      15  \n",
       "9      19  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subrname = \"politicaldiscussion\"\n",
    "#subrname = \"history\"\n",
    "subrname = \"quotes\"\n",
    "#subrname = \"changemyview\"\n",
    "\n",
    "#inpath = \"./\"\n",
    "#fname = \"dump_r-\"+subrname+\"_2019-09-12.pkl\"\n",
    "inpath = \"./subreddit_data/\"\n",
    "fname = inpath+\"r-\"+subrname+\"-export.pkl\"\n",
    "dfraw = pd.read_pickle(fname)\n",
    "\n",
    "# Filter out removed posts\n",
    "sel = dfraw[\"selftext\"].str.strip() == \"[removed]\"\n",
    "dfraw[\"selftext\"][sel] = \"\"\n",
    "dfraw[\"title+selftext\"] = dfraw[\"title\"]+\" \"+dfraw[\"selftext\"]\n",
    "\n",
    "# Filter out posts with less than 3 words\n",
    "dfraw[\"n_words\"] = dfraw[\"title+selftext\"][:].str.split()\n",
    "for i in range(0,len(dfraw[\"n_words\"])):\n",
    "    dfraw[\"n_words\"][i] = len(dfraw[\"n_words\"][i])\n",
    "\n",
    "sel = dfraw[\"n_words\"] > 3\n",
    "df = dfraw[sel]    \n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the submissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:03.320321Z",
     "start_time": "2019-09-18T17:07:03.316872Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''Function to lemmatize text'''\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:03.326302Z",
     "start_time": "2019-09-18T17:07:03.322880Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''Function to pre-process text'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:10.339197Z",
     "start_time": "2019-09-18T17:07:03.327869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess titles \n",
    "processed_subm = df[\"title+selftext\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:11.248088Z",
     "start_time": "2019-09-18T17:07:10.341265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Bag of Words \n",
    "dictionary = gensim.corpora.Dictionary(processed_subm)\n",
    "\n",
    "# Filter out irrelevant words \n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=5000)\n",
    "\n",
    "# For each tweet, create a dictionary reporting how many\n",
    "# words and how many times those words appear.\n",
    "bow_corpus = [dictionary.doc2bow(subm) for subm in processed_subm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: load LDA model or create it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:17.832836Z",
     "start_time": "2019-09-18T17:07:17.828817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory lda_models exists!\n"
     ]
    }
   ],
   "source": [
    "if path.exists(\"lda_models\"):\n",
    "    print(\"Directory lda_models exists!\")\n",
    "else:\n",
    "    !mkdir lda_models\n",
    "    print(\"Directory lda_models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:18.566572Z",
     "start_time": "2019-09-18T17:07:18.564083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose number of topics for LDA\n",
    "num_topics_lda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:45.985990Z",
     "start_time": "2019-09-18T17:07:19.555989Z"
    }
   },
   "outputs": [],
   "source": [
    "outpath = \"./lda_models/\"\n",
    "ldaoutfile = outpath+\"lda_model_\"+str(num_topics_lda)+\"topics_r-\"+subrname\n",
    "if path.exists(ldaoutfile):\n",
    "    print(ldaoutfile+\"model exists!\")\n",
    "    lda_model = gensim.models.LdaModel.load(ldaoutfile)\n",
    "else :\n",
    "    # Create Latent Dirichlet Allocation model with a given number of topics\n",
    "    ncores = mp.cpu_count()\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics_lda, id2word=dictionary, \\\n",
    "                passes=10, workers=ncores)\n",
    "    # Save LDA model to disc (it's expensive to regenerate)\n",
    "    lda_model.save(ldaoutfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform exploratory data analysis with the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:45.995653Z",
     "start_time": "2019-09-18T17:07:45.989745Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:46.019908Z",
     "start_time": "2019-09-18T17:07:45.998312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quot</td>\n",
       "      <td>good</td>\n",
       "      <td>love</td>\n",
       "      <td>life</td>\n",
       "      <td>live</td>\n",
       "      <td>think</td>\n",
       "      <td>world</td>\n",
       "      <td>peopl</td>\n",
       "      <td>life</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>face</td>\n",
       "      <td>good</td>\n",
       "      <td>want</td>\n",
       "      <td>life</td>\n",
       "      <td>great</td>\n",
       "      <td>peopl</td>\n",
       "      <td>time</td>\n",
       "      <td>lose</td>\n",
       "      <td>heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http</td>\n",
       "      <td>life</td>\n",
       "      <td>thing</td>\n",
       "      <td>http</td>\n",
       "      <td>quot</td>\n",
       "      <td>time</td>\n",
       "      <td>happi</td>\n",
       "      <td>go</td>\n",
       "      <td>fear</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time</td>\n",
       "      <td>best</td>\n",
       "      <td>like</td>\n",
       "      <td>quot</td>\n",
       "      <td>year</td>\n",
       "      <td>mean</td>\n",
       "      <td>http</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come</td>\n",
       "      <td>peopl</td>\n",
       "      <td>peopl</td>\n",
       "      <td>html</td>\n",
       "      <td>creat</td>\n",
       "      <td>peopl</td>\n",
       "      <td>want</td>\n",
       "      <td>think</td>\n",
       "      <td>mean</td>\n",
       "      <td>chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>think</td>\n",
       "      <td>better</td>\n",
       "      <td>look</td>\n",
       "      <td>blogspot</td>\n",
       "      <td>great</td>\n",
       "      <td>make</td>\n",
       "      <td>thing</td>\n",
       "      <td>thing</td>\n",
       "      <td>thing</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feel</td>\n",
       "      <td>unknown</td>\n",
       "      <td>hate</td>\n",
       "      <td>lifehealthrelax</td>\n",
       "      <td>experi</td>\n",
       "      <td>control</td>\n",
       "      <td>youtu</td>\n",
       "      <td>know</td>\n",
       "      <td>hand</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>best</td>\n",
       "      <td>idea</td>\n",
       "      <td>person</td>\n",
       "      <td>chang</td>\n",
       "      <td>time</td>\n",
       "      <td>want</td>\n",
       "      <td>know</td>\n",
       "      <td>happi</td>\n",
       "      <td>peopl</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>human</td>\n",
       "      <td>light</td>\n",
       "      <td>time</td>\n",
       "      <td>thing</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>better</td>\n",
       "      <td>teach</td>\n",
       "      <td>think</td>\n",
       "      <td>dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kind</td>\n",
       "      <td>shoot</td>\n",
       "      <td>quot</td>\n",
       "      <td>like</td>\n",
       "      <td>understand</td>\n",
       "      <td>like</td>\n",
       "      <td>quot</td>\n",
       "      <td>truth</td>\n",
       "      <td>robert</td>\n",
       "      <td>mind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic # 01 Topic # 02 Topic # 03       Topic # 04  Topic # 05 Topic # 06  \\\n",
       "0       quot       good       love             life        live      think   \n",
       "1       know       face       good             want        life      great   \n",
       "2       http       life      thing             http        quot       time   \n",
       "3       time       best       like             quot        year       mean   \n",
       "4       come      peopl      peopl             html       creat      peopl   \n",
       "5      think     better       look         blogspot       great       make   \n",
       "6       feel    unknown       hate  lifehealthrelax      experi    control   \n",
       "7       best       idea     person            chang        time       want   \n",
       "8      human      light       time            thing       world      world   \n",
       "9       kind      shoot       quot             like  understand       like   \n",
       "\n",
       "  Topic # 07 Topic # 08 Topic # 09 Topic # 10  \n",
       "0      world      peopl       life       love  \n",
       "1      peopl       time       lose      heart  \n",
       "2      happi         go       fear      peopl  \n",
       "3       http       like       know       know  \n",
       "4       want      think       mean      chang  \n",
       "5      thing      thing      thing       life  \n",
       "6      youtu       know       hand      world  \n",
       "7       know      happi      peopl       fall  \n",
       "8     better      teach      think      dream  \n",
       "9       quot      truth     robert       mind  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_topics_df = get_lda_topics(lda_model, num_topics_lda)\n",
    "lda_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:07:58.100976Z",
     "start_time": "2019-09-18T17:07:46.022311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test LDA model\n",
    "test_strings = {\"test_strings\" : list(df[\"title+selftext\"].values)}\n",
    "df_test_strings = pd.DataFrame(data = test_strings)\n",
    "\n",
    "# Same procedure as above\n",
    "processed_test_strings = df_test_strings[\"test_strings\"].map(preprocess)\n",
    "test_corpus = [dictionary.doc2bow(text) for text in processed_test_strings]\n",
    "raw = lda_model[test_corpus]\n",
    "shape = (len(raw),num_topics_lda)\n",
    "predicted_topic_lda = np.zeros(shape)\n",
    "for i,tups in enumerate(raw):\n",
    "    for j in range(0,len(tups)):\n",
    "        predicted_topic_lda[i,j] = tups[j][1]\n",
    "    #print(df_test_strings[\"test_strings\"].iloc[i],predicted_topic_lda[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: load NMF model or create it if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:03.688159Z",
     "start_time": "2019-09-18T17:08:03.683524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory nmf_models exists!\n"
     ]
    }
   ],
   "source": [
    "if path.exists(\"nmf_models\"):\n",
    "    print(\"Directory nmf_models exists!\")\n",
    "else:\n",
    "    !mkdir nmf_models\n",
    "    print(\"Directory nmf_models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:04.118388Z",
     "start_time": "2019-09-18T17:08:04.115885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of topics for NMF\n",
    "num_topics_nmf = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:04.916199Z",
     "start_time": "2019-09-18T17:08:04.668225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create sentences\n",
    "processed_subm_sentences = [' '.join(text) for text in processed_subm]\n",
    "\n",
    "# Word counts\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "x_counts = vectorizer.fit_transform(processed_subm_sentences)\n",
    "\n",
    "# TF-IDF transform\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(x_counts)\n",
    "\n",
    "# Normalize to unit length\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:05.636657Z",
     "start_time": "2019-09-18T17:08:05.206978Z"
    }
   },
   "outputs": [],
   "source": [
    "outpath = \"./nmf_models/\"\n",
    "nmfoutfile = outpath+\"nmf_model_\"+str(num_topics_lda)+\"topics_r-\"+subrname\n",
    "if path.exists(nmfoutfile):\n",
    "    print(nmfoutfile+\"model exists!\")\n",
    "    nmf_model = pickle.load(open(nmfoutfile, 'rb'))\n",
    "else :\n",
    "    # Create NMF model.\n",
    "    nmf_model = NMF(n_components=num_topics_nmf,init='nndsvd')\n",
    "    # Fit the model\n",
    "    nmf_model.fit(xtfidf_norm)\n",
    "    # Save NMF model to disc (it's expensive to regenerate)\n",
    "    pickle.dump(nmf_model, open(nmfoutfile, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:05.891577Z",
     "start_time": "2019-09-18T17:08:05.887137Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words, num_topics):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:06.729763Z",
     "start_time": "2019-09-18T17:08:06.709325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>want</td>\n",
       "      <td>quot</td>\n",
       "      <td>life</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>think</td>\n",
       "      <td>thing</td>\n",
       "      <td>live</td>\n",
       "      <td>happi</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time</td>\n",
       "      <td>best</td>\n",
       "      <td>chang</td>\n",
       "      <td>fall</td>\n",
       "      <td>go</td>\n",
       "      <td>chang</td>\n",
       "      <td>good</td>\n",
       "      <td>right</td>\n",
       "      <td>make</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>need</td>\n",
       "      <td>inspir</td>\n",
       "      <td>death</td>\n",
       "      <td>hate</td>\n",
       "      <td>say</td>\n",
       "      <td>care</td>\n",
       "      <td>mean</td>\n",
       "      <td>learn</td>\n",
       "      <td>money</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>georg</td>\n",
       "      <td>motiv</td>\n",
       "      <td>short</td>\n",
       "      <td>heart</td>\n",
       "      <td>person</td>\n",
       "      <td>start</td>\n",
       "      <td>beauti</td>\n",
       "      <td>dream</td>\n",
       "      <td>depend</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>help</td>\n",
       "      <td>go</td>\n",
       "      <td>william</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>littl</td>\n",
       "      <td>best</td>\n",
       "      <td>west</td>\n",
       "      <td>need</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chang</td>\n",
       "      <td>http</td>\n",
       "      <td>creat</td>\n",
       "      <td>croft</td>\n",
       "      <td>truth</td>\n",
       "      <td>time</td>\n",
       "      <td>happen</td>\n",
       "      <td>bear</td>\n",
       "      <td>unhappi</td>\n",
       "      <td>tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heaven</td>\n",
       "      <td>famou</td>\n",
       "      <td>experi</td>\n",
       "      <td>fear</td>\n",
       "      <td>true</td>\n",
       "      <td>wrong</td>\n",
       "      <td>chang</td>\n",
       "      <td>die</td>\n",
       "      <td>pursuit</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>person</td>\n",
       "      <td>say</td>\n",
       "      <td>easi</td>\n",
       "      <td>need</td>\n",
       "      <td>knowledg</td>\n",
       "      <td>world</td>\n",
       "      <td>better</td>\n",
       "      <td>tomorrow</td>\n",
       "      <td>birthday</td>\n",
       "      <td>believ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>everybodi</td>\n",
       "      <td>look</td>\n",
       "      <td>like</td>\n",
       "      <td>save</td>\n",
       "      <td>come</td>\n",
       "      <td>cooley</td>\n",
       "      <td>littl</td>\n",
       "      <td>forev</td>\n",
       "      <td>famili</td>\n",
       "      <td>chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>help</td>\n",
       "      <td>rememb</td>\n",
       "      <td>time</td>\n",
       "      <td>feel</td>\n",
       "      <td>understand</td>\n",
       "      <td>see</td>\n",
       "      <td>great</td>\n",
       "      <td>nietzsch</td>\n",
       "      <td>sagan</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic # 01 Topic # 02 Topic # 03 Topic # 04  Topic # 05 Topic # 06  \\\n",
       "0       want       quot       life       love        know      think   \n",
       "1       time       best      chang       fall          go      chang   \n",
       "2       need     inspir      death       hate         say       care   \n",
       "3      georg      motiv      short      heart      person      start   \n",
       "4      world       help         go    william      wisdom      littl   \n",
       "5      chang       http      creat      croft       truth       time   \n",
       "6     heaven      famou     experi       fear        true      wrong   \n",
       "7     person        say       easi       need    knowledg      world   \n",
       "8  everybodi       look       like       save        come     cooley   \n",
       "9       help     rememb       time       feel  understand        see   \n",
       "\n",
       "  Topic # 07 Topic # 08 Topic # 09 Topic # 10  \n",
       "0      thing       live      happi      peopl  \n",
       "1       good      right       make    unknown  \n",
       "2       mean      learn      money       like  \n",
       "3     beauti      dream     depend      world  \n",
       "4       best       west       need       feel  \n",
       "5     happen       bear    unhappi       tell  \n",
       "6      chang        die    pursuit       time  \n",
       "7     better   tomorrow   birthday     believ  \n",
       "8      littl      forev     famili      chang  \n",
       "9      great   nietzsch      sagan       care  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_topics_df = get_nmf_topics(nmf_model, 10, num_topics_nmf)\n",
    "nmf_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:14.308790Z",
     "start_time": "2019-09-18T17:08:07.436698Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Test NMF model\n",
    "test_strings = {\"test_strings\" : df[\"title+selftext\"].values}\n",
    "df_test_strings = pd.DataFrame(data = test_strings)\n",
    "\n",
    "# Same procedure as above\n",
    "processed_test_strings = df_test_strings[\"test_strings\"].map(preprocess)\n",
    "test_sentences = [' '.join(text) for text in processed_test_strings]\n",
    "x_test_counts = vectorizer.transform(test_sentences)\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\n",
    "xtfidf_test_norm = normalize(x_test_tfidf, norm='l1', axis=1)\n",
    "\n",
    "y = nmf_model.transform(xtfidf_test_norm)\n",
    "predicted_topic_nmf = normalize(y, norm='l1', axis=1)\n",
    "#for i in range(0,len(predicted_topic_nmf)):\n",
    "#    print(df_test_strings[\"test_strings\"].iloc[i],predicted_topic_nmf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: perform regression to predict popularity and controversiality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Popularity and Controversiality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:14.330715Z",
     "start_time": "2019-09-18T17:08:14.311132Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of Popular Posts =  0.11686968512985521\n",
      "Fraction of Controversial Posts =  0.19834520799816135\n"
     ]
    }
   ],
   "source": [
    "# Build labels and features\n",
    "\n",
    "features = predicted_topic_nmf\n",
    "\n",
    "# Popularity of a post\n",
    "p = [0] * len(df[\"numComms\"].values)\n",
    "popular = np.array(p); del p\n",
    "cutoff_pop = np.quantile(popularity,q=0.5)\n",
    "selp0 = df[\"numComms\"].values > cutoff_pop\n",
    "popular[selp0] = 1\n",
    "\n",
    "# Controversiality of a post\n",
    "c = [0] * len(df[\"upvote_ratio\"].values)\n",
    "controversial = np.array(c); del c\n",
    "selc0 = np.logical_and(df[\"upvote_ratio\"].values.astype(float) > 0.25, \n",
    "                    df[\"upvote_ratio\"].values.astype(float) < 0.75)\n",
    "controversial[selc0] = 1\n",
    "selc1 = np.logical_or(df[\"upvote_ratio\"].values.astype(float) < 0.25, \n",
    "                    df[\"upvote_ratio\"].values.astype(float) > 0.75)\n",
    "\n",
    "\n",
    "print(\"Fraction of Popular Posts = \",float(len(popular[selp0])/len(popular)))\n",
    "print(\"Fraction of Controversial Posts = \",float(len(controversial[selc0])/len(controversial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T22:18:40.058436Z",
     "start_time": "2019-09-17T22:18:40.055549Z"
    }
   },
   "source": [
    "## Split in training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:14.349223Z",
     "start_time": "2019-09-18T17:08:14.334354Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "train_size = 1.0 - test_size - validation_size\n",
    "\n",
    "X, featurep_test, Y, popular_test = \\\n",
    "    skmodsel.train_test_split(features,popular,test_size=test_size)\n",
    "featurep_train, featurep_validation, popular_train, popular_validation = \\\n",
    "    skmodsel.train_test_split(features,popular,test_size=validation_size/(1.0-test_size))\n",
    "\n",
    "X, featurec_test, Y, controversial_test = \\\n",
    "    skmodsel.train_test_split(features,controversial,test_size=test_size)\n",
    "featurec_train, featurec_validation, controversial_train, controversial_validation = \\\n",
    "    skmodsel.train_test_split(features,controversial,test_size=validation_size/(1.0-test_size))\n",
    "\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:08:14.422581Z",
     "start_time": "2019-09-18T17:08:14.351776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression for Popularity. Score on Training Set =  0.8844710028345975\n",
      "Logistic regression for Popularity. Score on Test Set =  0.8822177535191037\n",
      "Logistic regression for Popularity. Score on Validation Set =  0.8791082509767869 \n",
      "\n",
      "Logistic regression for Controversiality. Score  on Training Set =  0.8015015705201869\n",
      "Logistic regression for Controversiality. Score  on Training Set =  0.7997701809824763\n",
      "Logistic regression for Controversiality. Score  on Training Set =  0.8021144564467938 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "popular_logregmodel = popular_logregmodel.fit(featurep_train,popular_train)\n",
    "controversial_logregmodel = controversial_logregmodel.fit(featurec_train,controversial_train)\n",
    "\n",
    "print(\"Logistic regression for Popularity. Score on Training Set = \", \\\n",
    "      popular_logregmodel.score(featurep_train,popular_train))\n",
    "print(\"Logistic regression for Popularity. Score on Test Set = \", \\\n",
    "      popular_logregmodel.score(featurep_test,popular_test))\n",
    "print(\"Logistic regression for Popularity. Score on Validation Set = \", \\\n",
    "      popular_logregmodel.score(featurep_validation,popular_validation),\"\\n\")\n",
    "\n",
    "print(\"Logistic regression for Controversiality. Score  on Training Set = \", \\\n",
    "      controversial_logregmodel.score(featurec_train,controversial_train))\n",
    "print(\"Logistic regression for Controversiality. Score  on Training Set = \", \\\n",
    "      controversial_logregmodel.score(featurec_test,controversial_test))\n",
    "print(\"Logistic regression for Controversiality. Score  on Training Set = \", \\\n",
    "      controversial_logregmodel.score(featurec_validation,controversial_validation),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production model creation and optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
